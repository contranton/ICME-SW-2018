% Created 2018-01-10 Wed 15:00
\documentclass[11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\date{\today}
\title{ParallelProgramming}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.1.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents

\#+:TITLE Parallel Programming

\section{Day 1}
\label{sec-1}
Cindy is working on simulations! Quantum physics and lotsa cool stuff
Raison d'etre of parallel programming: Lots of data, hard simulations, lots of results

\subsection{Parallel Programming}
\label{sec-1-1}
The use of \emph{multiple processors} (CPUs, GPUS, devices, etc) for problem-solving.

Why is it useful? 
\begin{itemize}
\item Each core can perform the same operations, so with many more cores, many more operations can be performed.
\item Each core has its own memory
\end{itemize}

However\ldots{}
\begin{itemize}
\item Parallelizing code requires significant architectural changes
\item It doesn't \emph{magically} make serial code faster
\item Communication between units introduces significant overhead
\item Total time is determined by the slowest core. It becomes necessary to \emph{synchronize} workers.
\item Not all algorithms can be parallelized!
\end{itemize}


One of the main problems to solve in parallel computation is managing \emph{data transfer times}, i.e. the pipes between different cores. This introduces race conditions, locking, etc.

\subsubsection{{\bfseries\sffamily TODO} Example of an Unparallelizable system}
\label{sec-1-1-1}
x1 = f(x1, x2, x3)

\subsubsection{Hardware}
\label{sec-1-1-2}
In a common computer, most memory is \texttt{DRAM}, which has high capacity but high latency (60-200 cycles). However, there's a CPU cache that allows quick access to frequently-used memory (2-10 cycles depending on cache level).

Even simple changes in array access can change memory access efficiency! By manipulating array access it is possible to exploit the L caches.
\begin{enumerate}
\item Levels
\label{sec-1-1-2-1}
\begin{enumerate}
\item Core
\label{sec-1-1-2-1-1}
A single core has access to all three L levels, but is limited to performing serial/sequential operations.
\item Chip/Socket
\label{sec-1-1-2-1-2}

With multiple cores, each core has its own individual L1 and L2 cache, but the L3 cache is typically shared between these cores to allow \emph{shared memory access}. This is the first option for multiprocessing.

\item Motherboard
\label{sec-1-1-2-1-3}

Multiple chips in a single motherboard are connected through the DRAM memory exclusively and, in a different way than with individual cores, have Non-Uniform Memory Access (NUMA).

\item Computer Cluster
\label{sec-1-1-2-1-4}
Multiple motherboards in a rack are linked through \emph{switches} to other racks, which may be organized in different ways depending on the chosen topology.

\item Memory types
\label{sec-1-1-2-1-5}
\begin{itemize}
\item Sequential processing (cores) -- L Caches
\item Shared Memory (chips and motherboards)
\item Distributed Memory
\end{itemize}
\end{enumerate}
\end{enumerate}
\subsubsection{OpenMP}
\label{sec-1-1-3}
Application Programming Interface -- Manages memory and task assignment for \emph{shared memory}
\subsubsection{MPI}
\label{sec-1-1-4}
Message Passing Interface -- Communication between nodes in a \emph{distributed memory system}.
\subsubsection{Performance metrics}
\label{sec-1-1-5}
Speedup -- Performance gain in switching from serial to parallel: $s_p=\frac{T_\text{serial}}{T_\text{parallel}}$
Efficiency -- Speedup per core:
$E_p=\frac{s_p}{p}$

Allows us to gauge whether parallelizing is actually worth it. It's not always a worthwile investment, and this isn't even considering communication!

\subsubsection{Communication}
\label{sec-1-1-6}
\begin{itemize}
\item Latency: Cost of sending 0B -- time related
\item Bandwidth: Communication rate per second
\end{itemize}

Granularity: Level of parallelization

\subsubsection{Important ideas}
\label{sec-1-1-7}
Parallelism depends on \emph{processing} and \emph{communication}

\subsubsection{CACHE COHERENCE}
\label{sec-1-1-8}


\subsection{OpenMP}
\label{sec-1-2}
API to write multithreaded programs. It:
\begin{itemize}
\item Contains compilation directives and runtime libraries
\item Facilitates implemntation in Fortran, C, and C++
\item Support for multiple software and hardware architectures
\item Comes by default in gcc and clang
\end{itemize}

CHeck out the site at \href{http://www.openmp.com/}{OpenMP.com}


\begin{enumerate}
\item Fork and Join model
\label{sec-1-2-0-1}
A single-threaded application stops to perform a parallel action, complete it, and return to a single thread.

\subsubsection{Practical Problem: calculating Pi}
\label{sec-1-2-1}

$\pi$ can be estimated by calculating the integral $\int_0^1 \frac{4}{1+x^2}dx$ with the desired approximation accuracy through the Riemann sum $\sum_{i=0}^{n-1}f(x_1)\cdot\frac{1}{n}$

The magical OpenMP directive: \texttt{\#pragma omp parallel}



\subsubsection{Managing variable access}
\label{sec-1-2-2}
\begin{itemize}
\item Private
\item Shared
\item FirstPrivate: Variable created on beginning of parallel block
\item LastPrivate: Variable created at end of parallel block
\end{itemize}
\subsubsection{Thread scheduling}
\label{sec-1-2-3}
\section{Day 2}
\label{sec-2}
\subsection{Distributed memory}
\label{sec-2-1}
There's  no longer free access to memory located at different nodes -- data must be specifically requested.
\subsection{MPI}
\label{sec-2-2}
Message Passing Interface for distributed memory systems-- It's a IEEE \emph{standard}, not any particular piece of code.
\subsubsection{Calculating pi with distributed memory}
\label{sec-2-2-1}
We must explicitly send and receive the data managed by each node using \texttt{MPI\_SEND} and \texttt{MPI\_RECV}.

Parallel code \emph{must} be wrapped in \texttt{MPI\_Init(\&argc, \&argv)} and \texttt{MPI\_Finalize} 

Can define different rank sub-worlds
\subsubsection{1-1 communication}
\label{sec-2-2-2}

\texttt{MPI\_SEND(\&total, num\_vars=1, var\_type=MPI\_DOUBLE, to\_rank=1, tag, MPI\_COMM\_WORLD)}
\texttt{MPI\_RECV(\&sum, num\_vars=1, var\_type=MPI\_DOUBLE, from\_rank=0, tag, MPI\_COMM\_WORLD, status)}

Deadlocking

\subsection{Task management}
\label{sec-2-3}
% Emacs 25.1.1 (Org mode 8.2.10)
\end{document}
