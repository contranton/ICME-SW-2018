% Created 2018-01-12 Fri 19:40
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\setlength{\parindent}{1cm}
\date{\today}
\title{Day 4}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.1.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents

\section*{Random Forest and other Ensemble methods}
\label{sec-1}
\subsection*{Monte Carlo methods}
\label{sec-1-1}
Many problems do not have closed form solutions, e.g. the mean of the sum of two Negative-Binomial distributed random variables.
\subsection*{Bootstrap}
\label{sec-1-2}
It's physically imposible for us to pull ourselves up from our bootstraps -- however, it's viable to use the same data to generate more data! Useful when we don't have enough data for Monte Carlo methods!

Sample from distributions generated from known data, i.e. sample from the \emph{histograms}! This isn't any more complicated than simply sampling from the dataset. However, we must take care to put our samples \emph{back} into the original set else we change the shape of the distribution. \uline{Sampling with replacement}.

It doesn't matter that we end up with duplicates in the sampled dataset.
\subsection*{Bagging}
\label{sec-1-3}
\emph{Bootstrap aggregation}

Consider CARTS -- a different subset of the data might result in a widely differing tree from that generated from another subset. They're \emph{unstable}, even though the predictions aren't that much different.

The concept is to generate multiple predictors from a lot of bootstrapped datasets and aggregate their predictions. The reason this works is that erroneous answers are typically \emph{equally erroneous}, but those answers that are slightly more correct are correct in consensus!

A downside of this method is that we lose interpretability. A single tree is easy to understand, but how a thousand trees affect the prediction is quite a bit harder.
\subsubsection*{Out-of-bag cross validation!}
\label{sec-1-3-1}
There are some samples that are not included in the bootstrapped dataset --typically 1/3-- which can be naturally used for cross-validation.
\subsection*{Boosting}
\label{sec-1-4}
Fitting additional predictors to residuals, i.e. the \emph{errors}, from initial predictions. We adjust predictions using a percentage (decided with a learning rate $\lambda$) of some known errors.

Parameters:
\begin{itemize}
\item Tree number
\item Tree depth
\item $\lambda$ value
\end{itemize}
\subsection*{Random Forests}
\label{sec-1-5}
Bagged trees only have variability in the way that data is selected for the training. Thus, they can still be correlated amongst themselves, limiting their usefulness.

\emph{Random forests} introduce aditional randomness by \textbf{enforcing} the use of different splitting variables by randomly disallowing some percentage (chosen using CV). For example, bagged trees might be constantly misled into choosing a variable like 'age' over some other one, not allowing the model to find some better variable. This is the same idea as the concept of influenced, non-independent voters.

Because of this, each \emph{individual} tree tends to be worse, but the \emph{aggregate} is far better.

We still get OOB Cross-Validation!
\subsubsection*{Variable importance measurements}
\label{sec-1-5-1}
Measure whether some variable is important or not.
\begin{itemize}
\item Metric 1
\label{sec-1-5-1-1}
Average the total error decrease after a split involving this variable over all trees.
\item Metric 2
\label{sec-1-5-1-2}
Shuffle \emph{only the studied variable} and check whether prediction error increases. If it does, it turns out the variable was important!
\end{itemize}
\subsubsection*{Advantages}
\label{sec-1-5-2}
\begin{itemize}
\item Very robust under non-informative variables -- less variance
\item Less prone to overfitting
\item No need for pruning!
\item Built-in cross validation sets with OOB
\end{itemize}
\subsubsection*{DIsadvantages}
\label{sec-1-5-3}
\begin{itemize}
\item Hard to capture additive effects
\item Not interpretable
\end{itemize}
\section*{SVM extras}
\label{sec-2}
\subsection*{Imbalanced classes}
\label{sec-2-1}
One class occurs far more often than the other, e.g. in fraud detection, medical databses.

Problematic because algorithms work best on even databases, and there's typically poor performance on underrepresented the class.

How do we imporove performance?
\begin{itemize}
\item Get more data for underrepresented class!
\item Class weighting
\begin{itemize}
\item Penalize more harshly making an error on the smaller class
\end{itemize}
\item Special sampling methods
\begin{itemize}
\item Modify training observations to balance class sizes, tipically by undersampling (+) or oversampling (-). This, however, may throw away important training observations or end up severly overfitting the model.
\end{itemize}
\end{itemize}
\subsubsection*{Classification performance}
\label{sec-2-1-1}
\section*{Machine Learning for Text Data}
\label{sec-3}
\subsection*{Non-negative matrix decomposition}
\label{sec-3-1}
Non-negative matrix factorization $X=WH,W>=0,H>=0$ found by solving an optimization problem. $W$ becomes our basis conversion matrix, i.e. our dictionary, and $H$ contains the coefficients that represent the data.

\emph{Metafeatures}!

\subsubsection*{{\bfseries\sffamily TODO} Advantages:}
\label{sec-3-1-1}
\begin{itemize}
\item Interpretability thanks to non-negativity
\end{itemize}

\subsection*{Bag-of-Words model}
\label{sec-3-2}
Only store word frequency information.

\subsection*{N-gram}
\label{sec-3-3}
Contiguous sequence of words/characters, frequency counted similar to B.o.W. model.

\subsection*{Term-Document Matrix}
\label{sec-3-4}
A term may be either a word or a gram, representing each row in the matrix, whereby every column stores the frequency count for each term in some document

Red fish blue fish example

\subsubsection*{{\bfseries\sffamily TODO} Red fish blue fish example}
\label{sec-3-4-1}
\section*{Review Questions}
\label{sec-4}
\begin{itemize}
\item In highly variable data, it's best to use restrictive models to mitigate overfitting. Only use more flexibility when dealing with data naturally of that kind, i.e. non-linear data.
\end{itemize}
\section*{Crash Course on Neural Networks and Deep Learning}
\label{sec-5}
\subsection*{Neural Networks}
\label{sec-5-1}
Neural networks are modeled after physical brain neurons, but only inspirationally. Modern methods have lost most physical analogy.

Outputs of nodes are weighted differently as they become the inputs for other nodes.

We use non-linear \emph{activation functions} like ReLU, sigmoids, or hyperbolic tangents, so as to make the behavior of the network more complicated than a simple linear combination of weights.

Gradient descent is used to train the weights into minimzing some loss function assosciated with training data and predictions made. By itself, gradient descent is prone to not reaching global minima, though this can be useful for avoiding overfitting.

The mathematical theory of neural networks is severly underdeveloped! We use them from a technical perspective without understanding why they perform particularly well on certain datasets.

It's a field of tricks!

Regularization via \emph{early stopping} -- walk towards the minimum but don't actually reach it -- or \emph{dropout}, which motivates individual nodes to better respond to structures by themselves, rather than relying on other nodes'  behavior.

\subsection*{Deep Learning}
\label{sec-5-2}
Many many layers, many many parameters.

Only usable on \emph{obscenely} large datasets.
% Emacs 25.1.1 (Org mode 8.2.10)
\end{document}
