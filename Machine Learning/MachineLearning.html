<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Machine learning</title>
<!-- 2018-01-13 Sat 14:56 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Machine learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Roadmap</a></li>
<li><a href="#sec-2">Day 1</a>
<ul>
<li><a href="#sec-2-1">Introduction to Machine Learning</a>
<ul>
<li><a href="#sec-2-1-1">Available Tools</a></li>
<li><a href="#sec-2-1-2">Big idea</a></li>
<li><a href="#sec-2-1-3">Model types</a></li>
<li><a href="#sec-2-1-4">Accuracy vs Interpretability tradeoff</a></li>
<li><a href="#sec-2-1-5">Main Categories of Models</a></li>
</ul>
</li>
<li><a href="#sec-2-2">Introduction to Unsupervised Learning</a>
<ul>
<li><a href="#sec-2-2-1">Clustering</a></li>
<li><a href="#sec-2-2-2">Dimensionality Reduction</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-3">Day 2</a>
<ul>
<li><a href="#sec-3-1">Measuring algorithm performance</a>
<ul>
<li><a href="#sec-3-1-1">Cross Validation</a></li>
</ul>
</li>
<li><a href="#sec-3-2">Classification</a>
<ul>
<li><a href="#sec-3-2-1">K-Nearest Neighbor classifier</a></li>
<li><a href="#sec-3-2-2">Regression</a></li>
</ul>
</li>
<li><a href="#sec-3-3">Cross-Validation and Regularization</a>
<ul>
<li><a href="#sec-3-3-1">Cross-Validation</a></li>
<li><a href="#sec-3-3-2">Regularization</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-4">Day 3</a>
<ul>
<li><a href="#sec-4-1">Back to Regularization</a>
<ul>
<li><a href="#sec-4-1-1">Ridge Regression vs. Lasso &#x2013; Sparsity</a></li>
<li><a href="#sec-4-1-2">Bayesian priors</a></li>
<li><a href="#sec-4-1-3"><span class="todo TODO">TODO</span> Elastic Net</a></li>
<li><a href="#sec-4-1-4">Principal Components Regression</a></li>
</ul>
</li>
<li><a href="#sec-4-2">Support Vector Machines</a>
<ul>
<li><a href="#sec-4-2-1">Maximal Margin Classifier</a></li>
<li><a href="#sec-4-2-2">Support Vector Classifier</a></li>
<li><a href="#sec-4-2-3">Support Vector Machines</a></li>
<li><a href="#sec-4-2-4">SVMs for more classes?</a></li>
</ul>
</li>
<li><a href="#sec-4-3">Imputation on Missing Data</a>
<ul>
<li><a href="#sec-4-3-1">Imputation</a></li>
</ul>
</li>
<li><a href="#sec-4-4">Decision Trees &#x2013; Classification and Regression Trees (CART)</a>
<ul>
<li><a href="#sec-4-4-1">Creation algorithms</a></li>
<li><a href="#sec-4-4-2">Depth of the tree?</a></li>
<li><a href="#sec-4-4-3">Advantages</a></li>
<li><a href="#sec-4-4-4">Disadvantages</a></li>
</ul>
</li>
<li><a href="#sec-4-5">Ensemble Methods</a></li>
</ul>
</li>
<li><a href="#sec-5">Day 4</a>
<ul>
<li><a href="#sec-5-1">Random Forest and other Ensemble methods</a>
<ul>
<li><a href="#sec-5-1-1">Monte Carlo methods</a></li>
<li><a href="#sec-5-1-2">Bootstrap</a></li>
<li><a href="#sec-5-1-3">Bagging</a></li>
<li><a href="#sec-5-1-4">Boosting</a></li>
<li><a href="#sec-5-1-5">Random Forests</a></li>
</ul>
</li>
<li><a href="#sec-5-2">SVM extras</a>
<ul>
<li><a href="#sec-5-2-1">Imbalanced classes</a></li>
</ul>
</li>
<li><a href="#sec-5-3">Machine Learning for Text Data</a>
<ul>
<li><a href="#sec-5-3-1">Non-negative matrix decomposition</a></li>
<li><a href="#sec-5-3-2">Bag-of-Words model</a></li>
<li><a href="#sec-5-3-3">N-gram</a></li>
<li><a href="#sec-5-3-4">Term-Document Matrix</a></li>
</ul>
</li>
<li><a href="#sec-5-4">Review Questions</a></li>
<li><a href="#sec-5-5">Crash Course on Neural Networks and Deep Learning</a>
<ul>
<li><a href="#sec-5-5-1">Neural Networks</a></li>
<li><a href="#sec-5-5-2">Deep Learning</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Roadmap</h2>
<div class="outline-text-2" id="text-1">
<div class="org-src-container">

<pre class="src src-plantuml">if "Do you have labeled data?"
then --&gt; "Supervised Learning"
else --&gt; "Unsupervised Learning"
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Day 1</h2>
<div class="outline-text-2" id="text-2">
<p>
Focus on supervised and unsupervised learning
</p>
</div>
<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Introduction to Machine Learning</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1">Available Tools</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li>R for statistical computing
</li>
<li>Python packages such as scikit-learn or tensorflow
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2">Big idea</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
Find a model f such that for known vectors X and Y, \(Y=f(X)+\varepsilon\)
</p>
</div>
</div>
<div id="outline-container-sec-2-1-3" class="outline-4">
<h4 id="sec-2-1-3">Model types</h4>
<div class="outline-text-4" id="text-2-1-3">
</div><ul class="org-ul"><li><a id="sec-2-1-3-1" name="sec-2-1-3-1"></a>Parametric Models<br  /><div class="outline-text-5" id="text-2-1-3-1">
<p>
We're given a function with a set of parameters, and when training we only have to estimate the parameters instead of an arbitrary function.
</p>

<p>
Advantages: Easier to estimate
Disadvantages: May not be representative of the real \(f\)
</p>
</div>
</li>

<li><a id="sec-2-1-3-2" name="sec-2-1-3-2"></a>Non-parametric Models<br  /><div class="outline-text-5" id="text-2-1-3-2">
<p>
Make no assumptions about \(f\) and fit it as closely as possible without overfitting. Example: Spline fitting
</p>

<p>
Advantages: Captures local features very well
Disadvantages: Risk of overfitting; requires large dataset
</p>
</div>
</li></ul>
</div>
<div id="outline-container-sec-2-1-4" class="outline-4">
<h4 id="sec-2-1-4">Accuracy vs Interpretability tradeoff</h4>
</div>

<div id="outline-container-sec-2-1-5" class="outline-4">
<h4 id="sec-2-1-5">Main Categories of Models</h4>
<div class="outline-text-4" id="text-2-1-5">
</div><ul class="org-ul"><li><a id="sec-2-1-5-1" name="sec-2-1-5-1"></a>Supervised Learning<br  /><div class="outline-text-5" id="text-2-1-5-1">
<p>
We have input data and the associated response. We have both \(X^(i)\) and \(Y^(i)\)
</p>
</div>
<ul class="org-ul"><li><a id="sec-2-1-5-1-1" name="sec-2-1-5-1-1"></a>Regression<br  /><div class="outline-text-6" id="text-2-1-5-1-1">
<p>
Quantitative output,
</p>
</div>
</li>
<li><a id="sec-2-1-5-1-2" name="sec-2-1-5-1-2"></a>Classification<br  /><div class="outline-text-6" id="text-2-1-5-1-2">
<p>
Qualitative (categorical) output. "Whether-or-not" problems. May be formulated as a regression problem linked to <i>probabilities</i> of belonging to some category.
</p>
</div>
</li></ul>
</li>
<li><a id="sec-2-1-5-2" name="sec-2-1-5-2"></a>Unsupervised Learning<br  /><div class="outline-text-5" id="text-2-1-5-2">
<p>
We have the input data but no response data, i.e. only $X<sup>(i)</sup> not $Y<sup>(i)</sup>
</p>
</div>
<ul class="org-ul"><li><a id="sec-2-1-5-2-1" name="sec-2-1-5-2-1"></a>Clustering<br  /><div class="outline-text-6" id="text-2-1-5-2-1">
<p>
<i>Partition</i> data into subsets that share common characteristics
</p>
</div>
</li>
<li><a id="sec-2-1-5-2-2" name="sec-2-1-5-2-2"></a>Dimensionality reduction<br  /><div class="outline-text-6" id="text-2-1-5-2-2">
<p>
Create <i>new features</i> that best characterize high-dimensional data
</p>
</div>
</li></ul>
</li></ul>
</div>
</div>


<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Introduction to Unsupervised Learning</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Goal: Find patterns/properties, usually for visualizing <i>and interpreting</i> high-D data.
</p>
</div>

<ul class="org-ul"><li><a id="sec-2-2-0-1" name="sec-2-2-0-1"></a>Sample applications<br  /><div class="outline-text-5" id="text-2-2-0-1">
<ul class="org-ul">
<li>Recovering compact representations (bases) for high dimensional data
</li>
</ul>
</div>
</li>

<li><a id="sec-2-2-0-2" name="sec-2-2-0-2"></a>Challenges<br  /><div class="outline-text-5" id="text-2-2-0-2">
<p>
This is exploratory data analysis &#x2013; We don't know what the right answer is and our goal is never clearly defined.
</p>
</div>
</li>

<div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1">Clustering</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
Ways of finding subgroups within the data without any man-made labels.
</p>

<p>
Careful with confusing it with classification. Classification is the process of <i>selecting</i> a subgroup for some new element, whereas clustering is the process of <i>finding</i> the subgroups.
</p>
</div>

<ul class="org-ul"><li><a id="sec-2-2-1-1" name="sec-2-2-1-1"></a>Types of clustering models<br  /><div class="outline-text-5" id="text-2-2-1-1">
<ul class="org-ul">
<li>Centroid-based clusters
</li>
<li>Hierarchical
</li>
</ul>
</div>
</li>

<li><a id="sec-2-2-1-2" name="sec-2-2-1-2"></a>Hierarchical clustering<br  /><div class="outline-text-5" id="text-2-2-1-2">
<p>
Turns data into tree-like representations organized by the <i>distance</i> between different nodes. By specifying a cut-off depth we may manipulate the resolution of the clusters.
</p>

<p>
How do you calculate the distance between clusters?
</p>
<ul class="org-ul">
<li>Single-linkage: Calculate using the closest pair <i>only</i>.
</li>
<li>Complete-linkage: Calculate using the furthest pair <i>only</i>.
</li>
<li>Average linkage: Calculate using the distance between all combinations of nodes.
</li>
</ul>
</div>
</li>

<li><a id="sec-2-2-1-3" name="sec-2-2-1-3"></a>K-means clustering<br  /><div class="outline-text-5" id="text-2-2-1-3">
<p>
Each of \(K\) clusters is defined by a <i>centroid vector</i>, and each observation is assigned to a single cluster through the nearest centroid. As an input it requires the number \(K\) of desired clusters.
</p>

<p>
Measure of similarity through the Eucledian distance.
</p>

<p>
Goal is to minimze <i>within-cluster variation</i> \(J\) through least-squares optimization.
</p>
</div>

<ul class="org-ul"><li><a id="sec-2-2-1-3-1" name="sec-2-2-1-3-1"></a>Basics of the algorithm<br  /><div class="outline-text-6" id="text-2-2-1-3-1">
<p>
At each step:
</p>
<ol class="org-ol">
<li>Define initial cluster centroids
</li>
<li>Partition data by assigning <i>each sample</i> to the nearest centroid (2-norm)
</li>
<li><b>Recalculate</b> the centroids within each partition
</li>
</ol>
<p>
Repeat 2&amp;3 until clusters are invariant.
</p>
<div class="org-src-container">

<pre class="src src-python">while centroid_displacement_from_previous_iteration() &gt; eps:
    (sample.select_nearest_centroid() for sample in samples)
    recompute_centroids()
</pre>
</div>
</div>
</li>

<li><a id="sec-2-2-1-3-2" name="sec-2-2-1-3-2"></a>Initialization<br  /><div class="outline-text-6" id="text-2-2-1-3-2">
<p>
Poor initialization may lead to improper clustering!
</p>

<p>
Different ways of doing it:
</p>
<ul class="org-ul">
<li>Random selection of K centroids
</li>
<li>Random partition of data
</li>
<li>Select K points that are <i>mutually far apart</i>
</li>
<li>Domain-knowledge
</li>
<li>Initialize using results from some other method
</li>
<li>Average over many clustering runs
</li>
</ul>
</div>
</li>


<li><a id="sec-2-2-1-3-3" name="sec-2-2-1-3-3"></a>Cluster number<br  /><div class="outline-text-6" id="text-2-2-1-3-3">
<p>
We may have domain knowledge to inform a proper choice of K, but usually it can be determined from the data itself.
</p>

<p>
We <i>can't</i> just pick the \(K\) that minimizes \(J\) since the ideal clustering is then to make <i>each point its own cluster</i>.
</p>

<p>
A heuristic methods runs different K values and plots the corresponding J value. We can look at the "elbow" of the \(K\) vs. \(J\) plot, at which we don't get any additional benefit by adding more 
</p>
</div>
</li>

<li><a id="sec-2-2-1-3-4" name="sec-2-2-1-3-4"></a>Advantages<br  /><div class="outline-text-6" id="text-2-2-1-3-4">
<ul class="org-ul">
<li>Easy to implement
</li>
<li>Often converges in very few operations
</li>
<li>Can be applied on data with many features
</li>
</ul>
</div>
</li>

<li><a id="sec-2-2-1-3-5" name="sec-2-2-1-3-5"></a>Disadvantages<br  /><div class="outline-text-6" id="text-2-2-1-3-5">
<ul class="org-ul">
<li>We must chose \(K\), risking not having an optimal solution or being forced to perform multiple expensive runs.
</li>
<li>Iterative algorithm returns <i>local minima</i>.
</li>
<li>By using the 2-norm, it assumes that all clusters are spherical and about the same size.
</li>
<li>Very sensitive to outliers, as with any method that relies on means.
</li>
</ul>

<p>
However, there are many variants that fix some of these issues.
</p>
<ul class="org-ul">
<li>Running algorithm multiple times
</li>
<li>Using medians instead of means
</li>
<li>Require that the centroid <i>must be a data point</i>
<ul class="org-ul">
<li>Robust to outliers
</li>
<li>Flexible &#x2013; can use any similarity measure
</li>
<li>Computationally expensive to calculate the mean
</li>
</ul>
</li>
</ul>
</div>
</li>

<li><a id="sec-2-2-1-3-6" name="sec-2-2-1-3-6"></a>Possible modifications<br  /><div class="outline-text-6" id="text-2-2-1-3-6">
<p>
Many possible modifications, like weighing the distance by some metric. For example, if we know a particular subset of a dataset is noisy, we can downplay its importance by weighing its distance to the centroid less than we weigh other subsets.
</p>
</div>
</li></ul>
</li></ul>
</div>



<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2">Dimensionality Reduction</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
For visualization and data intuition purposes it's necessary to wrangle higher dimensions into human-understandable terms. This is basically the science of making good projections onto lower-dimensional spaces. We <i>always</i> lose information.
</p>
</div>


<ul class="org-ul"><li><a id="sec-2-2-2-1" name="sec-2-2-2-1"></a>Maximal Variance Projection (PCA)<br  /><div class="outline-text-5" id="text-2-2-2-1">
<p>
Usually we'll chose hyperplanes that <i>retain the spread</i> of the data, i.e. maximizing variance. This is <b>Principal Component Analysis</b>.
</p>

<p>
If we have a 2D oval cluster, projecting onto the line that goes through the major axis will maximize the variance. This line is found through either the:
</p>
<ul class="org-ul">
<li>Sample Covariance matrix: \(X^TX=VD^2V^T\)
</li>
<li>Singular Value Decomposition: \(X=UDV^T\)
</li>
</ul>
<p>
Where X: Data matrix, V: Eigenvalue matrix, D: Eigenvector matrix
</p>

<p>
That is, the eigenvector that corresponds to the <i>largest eigenvalue</i> is this best-projection line. These eigenvalues are the variances!
</p>

<p>
Check out the paper "Genes mirror geography within Europe". Very high-dimensional data, when projected on 2D using PCA, <i>approximates Europe's geography!</i>
</p>
</div>

<ul class="org-ul"><li><a id="sec-2-2-2-1-1" name="sec-2-2-2-1-1"></a>Steps:<br  /><div class="outline-text-6" id="text-2-2-2-1-1">
<ul class="org-ul">
<li>Centering (always) &#x2013; Subtract the centroid from each data point to center it at the origin.
</li>
<li>Normalizing (used for correlation PCA; not the same thing)
</li>
<li>Whitening (never) &#x2013;  Standarize units by dividing by standard deviations.
</li>
</ul>
</div>
</li>


<li><a id="sec-2-2-2-1-2" name="sec-2-2-2-1-2"></a>Differences with Linear Regression<br  /><div class="outline-text-6" id="text-2-2-2-1-2">
<p>
A linear regression minimizes the square of the <i>vertical</i> distance between some sample and the regressor. However, the PCA fit minimizes the total <i>euclidean</i> distance between data and fit.
</p>
</div>
</li>


<li><a id="sec-2-2-2-1-3" name="sec-2-2-2-1-3"></a>Choosing Principal Components<br  /><div class="outline-text-6" id="text-2-2-2-1-3">
<p>
We'll usually choose 2 PCs to visualize data in paper media or presentations.
</p>

<p>
We can measure how much of the variance is lost. Recall that the eigenvalues capture the variance of each PC eigenvector! If we plot a PC vs. \(\sigma^2\) histogram, we can look for steep drop-offs. If we don't find these, there's a lot of information we're losing by reducing dimensionality. In these cases, be careful of finding degenerate eigenvectors (equal eigenvalues) &#x2013; both of these must be present, as any linear combination of these is also valid
</p>
</div>

<ul class="org-ul"><li><a id="sec-2-2-2-1-3-1" name="sec-2-2-2-1-3-1"></a>Analytical Method<br  /><div class="outline-text-7" id="text-2-2-2-1-3-1">
<p>
Variance explained: \(W_k = \frac{1}{N}\sum_{i=1}^{k} {d_i^2}\) (Average square error)
</p>


<p>
Gap Statistic: \(\log W_k - \log W_k^*\)
</p>


<p>
\(W_k^*\) is the variation found from random data (same N) simulated inside some bounding region (bounding box, convex hull, etc). By plotting the gap statistic, we measure how much <i>better</i> our fit works on the data than with random data. The maximum gives us the optimum k.
</p>


<p>
This is also used in k-means with the \(J\) metric instead of \(W\).
</p>
</div>
</li></ul>
</li>


<li><a id="sec-2-2-2-1-4" name="sec-2-2-2-1-4"></a>Dangers<br  /><div class="outline-text-6" id="text-2-2-2-1-4">
<p>
The first PC is not always the best projection! Imagine a data set with parallel oblique clusters &#x2013; the first PC is parallel to these clusters, as it captures the most spread. However, by projecting onto it, the clusters merge and we'll think our data is completely structureless!
</p>
</div>
</li></ul>
</li>


<li><a id="sec-2-2-2-2" name="sec-2-2-2-2"></a>Self Organizing Maps<br  /><div class="outline-text-5" id="text-2-2-2-2">
<p>
If we find a hypercurve that best fits the data, it's possible to describe all the data with a single parameter. However, finding an arbitrary curve is a rather difficult problem.
</p>

<p>
If we define an initial straight curve (first principal component, usually) defined by N points \(m_k\), we iterate by making every sample point \(x_i\) <i>pull</i> the closest \(m_k\) towards itself, proportional to how far apart they are. This is described by the relation \(m_k \leftarrow m_k + \alpha\left(x_i - m_k\right)\). The fact that the pulling action is averaged over all the nearby data points is key for preserving the curve structure.
</p>

<p>
The parameter &alpha; is called the <i>learning rate</i>, and is usually best set to a small value to approach the solution smoothly through lots of iterations rather than jerking back and forth and risk accuracy.
</p>
</div>
</li>


<li><a id="sec-2-2-2-3" name="sec-2-2-2-3"></a>Multi-Dimensional Scaling (MDS)<br  /><div class="outline-text-5" id="text-2-2-2-3">
<p>
By defining distance metrics between variables (can be esoteric), we record these distances in a distance map which we can reconstruct using least-squares minimization. The idea of reconstructing maps from 'road distances', for example, is viable! The resulting map might be arbitrarily rotated, but it will capture geometrical relationships very well.
</p>


<p>
These are affine spaces in which we can't define an origin but we can define distances.
</p>


<p>
\(\min_{x_1,...,x_N} \sum_{i<j} \left(||x_i-x_j||-\delta_{ij}\right)^2\)
</p>
</div>
</li>


<li><a id="sec-2-2-2-4" name="sec-2-2-2-4"></a>Independent Component Analysis (ICA)<br  /><div class="outline-text-5" id="text-2-2-2-4">
<p>
Best used when data is a linear superposition of signals, like with audio. Let's assume we have three microphones in a room and we want to isolate each independent sound source out of three: TV, Radio, Speech.
</p>

<p>
We have a data matrix with each variable (microphone) over time which we'll whiten/normalize. For this we compute all PCs, and scale along each of these by the according eigenvalue, after which direcions become uncorrelated but not independent.
</p>

<p>
From this, we want to the lowest entropy projection, or that which is non-Gaussian. This is, that it has <i>structure</i> as measured by some entropy metric. Projecting on these will ensure that we maintain structure, rather than just variance.
</p>

<p>
An important limitation is that you can only find as many projections as you have 'measuring devices'.
</p>

<p>
This has applications in finding patterns in financial data, earth monitoring, etc.
</p>
</div>
</li></ul>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Day 2</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">Measuring algorithm performance</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Loss functions quantify cost of errors. E.g: Mean Squared Error (MSE) = \(\frac{1}{N}\sum\left(y_i-f(x_i)\right)^2\)
</p>

<p>
Keep in mind that we don't want to minimize the error on the data we have (that leads to overfitting. Rather, we want to minimize the error on <i>new data</i>, such that we build a model that <i>generalizes</i>, rather than just fit the trends.
</p>

<p>
In the end, we want to minimize the <i>expected loss</i> on future data.
</p>

<p>
Training error and Test error behave differently. As our model flexibility increases, the training error always decreases, but past a certain point the test error will actually begin to increase!
</p>

<p>
When the number of degrees of freedom approaches the size of the data we get into dangerous overfitting territory. Complex models like deep neural networks involve millions of parameters, though at the same time they require far larger datasets to train.
</p>
</div>
<div id="outline-container-sec-3-1-1" class="outline-4">
<h4 id="sec-3-1-1">Cross Validation</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
Divide the data into three subsets:
</p>
<ul class="org-ul">
<li>Training data: Subset used to learn the model
</li>
<li>Validation data: Subset used to estimate error for tuning or model selection
</li>
<li>Test data: Subset used to check model performance. This has not been previously been used on the model.
</li>
</ul>
</div>
<ul class="org-ul"><li><a id="sec-3-1-1-1" name="sec-3-1-1-1"></a>Bias vs. Variance Tradeoff<br  /><div class="outline-text-5" id="text-3-1-1-1">
<p>
The expectation of the error for the given estimator (i.e., model) is given by
</p>

<p>
\(\text{Variance(}f\hat\text{)} + \text{Bias(}f\hat\text{)}^2 + \text{Variance(}\varepsilon\text{)}\)
</p>

<p>
The bias represents the ability of the model to represent the actual trend. For example, trying to linearly fit a nonlinear model is biased towards the linear fit wheras a slightly more complex model would fit the data better.
</p>

<p>
Variance, on the other hand, quantifies how much the estimation varies with different datasets.
</p>

<p>
As we increase the DOFs, bias is reduced, but at the same time variance is increased.
</p>

<p>
Ideally, low bias <i>and</i> low variance imply a low test error.
</p>
</div>
</li></ul>
</div>
</div>
<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2">Classification</h3>
<div class="outline-text-3" id="text-3-2">
</div><div id="outline-container-sec-3-2-1" class="outline-4">
<h4 id="sec-3-2-1">K-Nearest Neighbor classifier</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
Basic idea: classify observations based on nearby labels.
</p>

<p>
The predicted class for a sample X is the most common class among its K nearest neighbors from the training set.
</p>

<p>
The probability of belonging to some class is quite easily \(\frac{\#\text{(class among neighbors)}}{K}\).
</p>

<p>
Decision boundaries are a useful way of visualizing the regions of classification as well as noting the complexity of some fit.
</p>

<p>
This is a <i>non-parametric</i> model, as we're not fitting an equation with fixed parameters.
</p>

<p>
Useful for imputating data!
</p>
</div>
<ul class="org-ul"><li><a id="sec-3-2-1-1" name="sec-3-2-1-1"></a>Choosing K<br  /><div class="outline-text-5" id="text-3-2-1-1">
<p>
With small K, we overfit and the decision boundary is very rough and jittery. However, with a large K the data might not be abundant enough to capture <i>any</i> trends.
</p>

<p>
Again, by minimizing the expected error we can choose the optimal K.
</p>
</div>
</li>
<li><a id="sec-3-2-1-2" name="sec-3-2-1-2"></a>Advantages<br  /><div class="outline-text-5" id="text-3-2-1-2">
<ul class="org-ul">
<li>Simple to implement
</li>
<li>Few tuning parameters (K, distance metric)
</li>
<li>Flexible, doesn't impose linear separability
</li>
</ul>
</div>
</li>
<li><a id="sec-3-2-1-3" name="sec-3-2-1-3"></a>Disadvantages<br  /><div class="outline-text-5" id="text-3-2-1-3">
<ul class="org-ul">
<li>Computationally expensive
</li>
<li>Sensitive to imbalanced datasets (larger classes smother the smaller ones)
</li>
<li>Sensitive to irrelevant inputs
</li>
</ul>
</div>
</li></ul>
</div>
<div id="outline-container-sec-3-2-2" class="outline-4">
<h4 id="sec-3-2-2">Regression</h4>
<div class="outline-text-4" id="text-3-2-2">
</div><ul class="org-ul"><li><a id="sec-3-2-2-1" name="sec-3-2-2-1"></a>Linear Regression<br  /><div class="outline-text-5" id="text-3-2-2-1">
<p>
It's a simple supervised learning method! Its advantage is <i>highly interpretable</i> as the slope parameters easily quantify the impact of individual variables.
</p>
</div>
<ul class="org-ul"><li><a id="sec-3-2-2-1-1" name="sec-3-2-2-1-1"></a>Simple Linear Regression<br  /><div class="outline-text-6" id="text-3-2-2-1-1">
<p>
Parametric model given by \(Y=\beta_0 + \beta_1 X + \varepsilon\).
</p>

<p>
We estimate $\hat&beta;<sub>0</sub> and $\hat&beta;<sub>1</sub> using training data to find a good fit, and for this we usually use the Mean Squared Error.
</p>

<p>
SLR in particular has a closed-form solution for the parameters that gives the best least-squares fit.
</p>
</div>
</li>
<li><a id="sec-3-2-2-1-2" name="sec-3-2-2-1-2"></a>Multiple Linear Regression<br  /><div class="outline-text-6" id="text-3-2-2-1-2">
<p>
We use more than one predictor (X) variable on the form Y=&beta;<sub>0</sub>+&beta;<sub>1</sub> X+&beta;<sub>2</sub> X<sub>2</sub>+&#x2026;
</p>

<p>
Using the magic of linear algebra, we may define X and &beta; vectors to set up the optimization problem \hat&beta; = argmin<sub>&beta;</sub> ||Y - X<sup>T</sup>&beta; ||<sup>2</sup>. This also has a closed-form solution (found through the <i>normal equations</i>) given by \(\beta = \left(X^TX\right)^{-1}X^TY\)
</p>
</div>
</li>

<li><a id="sec-3-2-2-1-3" name="sec-3-2-2-1-3"></a>Variations on Linear Regression<br  /><ul class="org-ul"><li><a id="sec-3-2-2-1-3-1" name="sec-3-2-2-1-3-1"></a>Weighted Linear Regression<br  /><div class="outline-text-7" id="text-3-2-2-1-3-1">
<p>
Not all data is created equal &#x2013; if we know a particular subset of data is less reliable, i.e. more noisy, we reduce its impact on the model through weighting.
</p>

<p>
Using a diagonal matrix \(W\) with elements $w<sub>i</sub> = \frac{1}{\sigma_i^2} where each &sigma;<sub>i</sub> may be esoteric or properly measured. With this, the problem becomes
</p>
</div>
</li>
<li><a id="sec-3-2-2-1-3-2" name="sec-3-2-2-1-3-2"></a>Locally weighted linear regression<br  /><div class="outline-text-7" id="text-3-2-2-1-3-2">
<p>
We pressume an <i>a priori</i> interest in some particular region of data. For some observation, we apply a gaussian weighing surrounding the point of interest to produce a sort of tangent line.
</p>

<p>
A new linear fit must be produced for <i>every</i> desired observation, and in the end this produces a piece-wise linear approximation to the trend curve.
</p>
</div>
</li></ul>
</li></ul>
</li>
<li><a id="sec-3-2-2-2" name="sec-3-2-2-2"></a>Logistic Regression<br  /><ul class="org-ul"><li><a id="sec-3-2-2-2-1" name="sec-3-2-2-2-1"></a>Qualitative inputs<br  /><div class="outline-text-6" id="text-3-2-2-2-1">
<p>
Class predictors like KNN are based on purely categorical data. However, it's possible to classify <i>using regression</i> by regressing probabilities of belonging to different classes.
</p>

<p>
When input data is qualitative, (e.g. "female", "male", "blue eyes", "brown eyes") it's easy to use binary variables that quantify this. However, it's dangerous to apply a linear regression with categories assigned to different numbers (i.e. symptoms: headache:1, seizure:2, stroke:3) to any categories as this introduces unnatural orderings that bias the fit.
</p>

<p>
The best way of dealing with categorical data is using <i>binary variables</i>. Either 1 or 0 whether the category is true or not.
</p>
</div>
</li>
<li><a id="sec-3-2-2-2-2" name="sec-3-2-2-2-2"></a>Description<br  /><div class="outline-text-6" id="text-3-2-2-2-2">
<p>
\(Y\) takes on two values: 0 or 1, and we estimate it with probabilities in the interval \([0-1]\). 
</p>

<p>
We use the sigmoid/logistic function \(\sigma(z)=\frac{1}{1+e^{-z}}\) that lies in the desired interval. By making \(z\) the linear regressor (Xs aren't forced to be binary), we can "squash" the data to perform categorical fitting.
</p>

<p>
By choosing the sub-space where \(\mathbb{P}=p\) we can define the decision boundary for the classification, so instead of fitting a line, we're fitting a sort of <i>step</i> function.
</p>
</div>
</li>
<li><a id="sec-3-2-2-2-3" name="sec-3-2-2-2-3"></a>Advantages<br  /><div class="outline-text-6" id="text-3-2-2-2-3">
<ul class="org-ul">
<li>Extension of Linear Regression
</li>
<li>Interpretability &#x2013; log-odds are linear
</li>
<li>No tuning of hyperparameters
</li>
</ul>
</div>
</li>
<li><a id="sec-3-2-2-2-4" name="sec-3-2-2-2-4"></a>Disadvantages<br  /><div class="outline-text-6" id="text-3-2-2-2-4">
<ul class="org-ul">
<li>Can't model complex decision boundaries
</li>
<li>May overfit in training data, though it can be mitigated with Regularization in the MLE method
</li>
<li>Problem <b>must</b> be formulated as binary classification
</li>
</ul>
</div>
</li></ul>
</li></ul>
</div>
</div>

<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3">Cross-Validation and Regularization</h3>
<div class="outline-text-3" id="text-3-3">
</div><div id="outline-container-sec-3-3-1" class="outline-4">
<h4 id="sec-3-3-1">Cross-Validation</h4>
<div class="outline-text-4" id="text-3-3-1">
</div><ul class="org-ul"><li><a id="sec-3-3-1-1" name="sec-3-3-1-1"></a>XKCD example with the jelly beans and medical research<br  /><div class="outline-text-5" id="text-3-3-1-1">
<p>
<a href="https://xkcd.com/882/">Statistical significance</a> has no meaning if we don't properly manage our p-values to account for the fact that we're bombarding the data with the same models. If we get results, it's probably because we arrived at the \(p%\) of times that we get it from pure chance!
</p>
</div>
</li>
<li><a id="sec-3-3-1-2" name="sec-3-3-1-2"></a><span class="todo TODO">TODO</span> Loss Functions<br  /><div class="outline-text-5" id="text-3-3-1-2">
<p>
In linear regression, why do we use the squared error instead of just the absolute error? Usually it's because it gives us a closed form solution.
</p>
</div>

<ul class="org-ul"><li><a id="sec-3-3-1-2-1" name="sec-3-3-1-2-1"></a>Squared Error<br  /><div class="outline-text-6" id="text-3-3-1-2-1">
<p>
\(\sum_j\left(Y-f(X)\right)^2\)
</p>

<p>
This is affected much more by outliers, as they make the square error far larger than the simple distance. This error function leads to the mean, which presents high sensitivity.
</p>
</div>
</li>
<li><a id="sec-3-3-1-2-2" name="sec-3-3-1-2-2"></a>Absolute Error<br  /><div class="outline-text-6" id="text-3-3-1-2-2">
<p>
\(\sum_j\left|Y-f(X)\right|\)
</p>

<p>
Since there's no further penalization of far-away terms, this loss function is less sensitive to outliers. This gives rise to the median, a far more stable central measure.
</p>
</div>
</li></ul>
</li>

<li><a id="sec-3-3-1-3" name="sec-3-3-1-3"></a><span class="todo TODO">TODO</span> Classification Loss Functions<br  /><div class="outline-text-5" id="text-3-3-1-3">
<p>
Indicator error (nuumber of times "I screwed up", i.e. classifying spam(1) as not-spam(0)).
</p>


<p>
"Flip" what we're saying: If we get something wrong 80% of the time, just flip the classification!
</p>
</div>

<ul class="org-ul"><li><a id="sec-3-3-1-3-1" name="sec-3-3-1-3-1"></a><span class="todo TODO">TODO</span> Misclassification<br  /><div class="outline-text-6" id="text-3-3-1-3-1">
<p>
Gini index of uniformity
</p>
</div>
</li></ul>
</li>

<li><a id="sec-3-3-1-4" name="sec-3-3-1-4"></a><span class="todo TODO">TODO</span> Supervised Learning Theory<br  /><div class="outline-text-5" id="text-3-3-1-4">
<p>
Given a loss function \(L(Y, \tilde f(X))\), our goal is to find \(\hat f = \argmin_{\tilde f} \mathbb{E}[L(Y, \tilde f(X))]\). That is, across the population \(X\), find the model \(\tilde f\) that minimizes the expected error.
</p>
</div>
</li>

<li><a id="sec-3-3-1-5" name="sec-3-3-1-5"></a>K-fold Cross Validation<br  /><div class="outline-text-5" id="text-3-3-1-5">
<p>
We divide our data into different subsets, some of which are used for testing and some of which are used to <i>validate</i> the model. We can reshuffle the dataset and do this multiple times to get an average.
</p>

<p>
Cross-Validation Error: (tests whether a <b><i>type</i></b> of model is a good fit, rather than a particular fit). This cannot be used to replace the necessity for validation sets!
</p>

<p>
N-fold cross-validation is the best validation method &#x2013; it leaves one sample out and uses everybody else to train and later validate one the one sample. It performs this N times, meaning it's very expensive for more complex models.
</p>

<p>
Good compromises are 5-fold and 10-fold cross valdiations.
</p>

<p>
<i>Types</i> of models can be understood as different parameters for a single model, e.g. 2 neighbors for KNN vs 3 neighbors, etc.
</p>
</div>
</li>

<li><a id="sec-3-3-1-6" name="sec-3-3-1-6"></a>Bias-Variance tradeoff<br  /><div class="outline-text-5" id="text-3-3-1-6">
<p>
See <a href="#sec-3-1-1-1">Previous Heading</a>.
</p>
</div>
</li>

<li><a id="sec-3-3-1-7" name="sec-3-3-1-7"></a>Learning Curves<br  /><div class="outline-text-5" id="text-3-3-1-7">
<p>
How do we answer the question: do we need a better model or do we need more data?
</p>

<p>
Compare the training error and Cross-validation error as the data size changes <i>with the same model</i>. As we increase the data size, the same fit isn't as able to fit newer data so the <i>training error increases</i> as the cross-validation error decreases and eventually bottoms out. These errors never actually cross, but when the curves flatten out it's time to get a better model.
</p>

<p>
In summary, big gap implies we need more data, small gaps imply it's time to improve the model.
</p>
</div>
</li>

<li><a id="sec-3-3-1-8" name="sec-3-3-1-8"></a>Common problems!!<br  /><div class="outline-text-5" id="text-3-3-1-8">
<p>
The test sets must <i>never be used until testing</i>. We can't use them to extract information that will affect how we develop the models.
</p>

<p>
Be careful with using too many different models on the same test set. Eventually, a model will <i>by chance</i> fit the model well.
</p>
</div>
</li>

<li><a id="sec-3-3-1-9" name="sec-3-3-1-9"></a><span class="todo TODO">TODO</span> Importance of the validation set<br  /><div class="outline-text-5" id="text-3-3-1-9">
<p>
Only, and only when we've exhausted the model space and found a model that might
</p>
</div>
</li>

<li><a id="sec-3-3-1-10" name="sec-3-3-1-10"></a>Training data issues<br  /><ul class="org-ul"><li><a id="sec-3-3-1-10-1" name="sec-3-3-1-10-1"></a>Data is biased!<br  /><div class="outline-text-6" id="text-3-3-1-10-1">
<p>
As an example, historical text incorporates sexism (4 times more male references to female ones, as well as other biases in content) that gets incorporated into automated systems unnoticed.
</p>

<blockquote>
<p>
Machine Learning is like money laundering for bias. It's a clean, mathematical apparatus that gives the status quo the aura of logical invenitability
&#x2013;Maciej Ceglowski
</p>
</blockquote>
</div>
</li></ul>
</li></ul>
</div>

<div id="outline-container-sec-3-3-2" class="outline-4">
<h4 id="sec-3-3-2">Regularization</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
"Too many cooks spoil the soup"
</p>

<p>
Many models for regression involve many variables or so-called "proprietary" variables &#x2013; functions of the other variables.
</p>
</div>

<ul class="org-ul"><li><a id="sec-3-3-2-1" name="sec-3-3-2-1"></a>Problems with models<br  /><ul class="org-ul"><li><a id="sec-3-3-2-1-1" name="sec-3-3-2-1-1"></a>More variables than samples<br  /><div class="outline-text-6" id="text-3-3-2-1-1">
<p>
Having too few samples for very high-dimensional models allow us to fit an infinitude of spaces. For example fitting two samples to a three dimensional model lets us fit infinitely many planes that pass through the line connecting the samples.
</p>
</div>
</li>
<li><a id="sec-3-3-2-1-2" name="sec-3-3-2-1-2"></a>Multiple-Colinearity<br  /><div class="outline-text-6" id="text-3-3-2-1-2">
<p>
What can go wrong when we have a few variables that are highly correlated between each other but not with the variable we're trying to predict? When performing a fit, these variables won't affect the predictor and might happen to cancel out mutually &#x2013; because of this, their weights might become disproportionally large, making it seem like these variables are actually very important when in fact they're not!
</p>
</div>
</li></ul>
</li>


<li><a id="sec-3-3-2-2" name="sec-3-3-2-2"></a>Ridge Regression<br  /><div class="outline-text-5" id="text-3-3-2-2">
<p>
Incorporate a 'penalized error' \(\lambda \left(\sum\beta_i^2\right)\). 
</p>

<p>
As &lambda; increases, the <i>bias</i> increases as we end up simplifying the model (&beta;<sub>i</sub> tend to zero), and the variance decreases as we always reach the same estimations.
</p>
</div>
</li>

<li><a id="sec-3-3-2-3" name="sec-3-3-2-3"></a>Lasso<br  /><div class="outline-text-5" id="text-3-3-2-3">
<p>
How do we choose which variables can be tuned to <i>exactly</i> zero?
</p>

<p>
"Least Absolute Shrinkage and Selection Operator"
</p>

<p>
Rather than squaring the betas, we use their absolute value. Thus, the penalty is \(\lambda\left(\sum|\beta_i|\right)\)
</p>

<p>
This becomes an L1 problem rather than an L2 problem used in Ridge Regression. It doesn't find smooth values very well but it approaches zero very reliably. Because of this, our data is able to become sparse much more reliably.
</p>
</div>
</li></ul>
</div>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Day 3</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1">Back to Regularization</h3>
<div class="outline-text-3" id="text-4-1">
</div><div id="outline-container-sec-4-1-1" class="outline-4">
<h4 id="sec-4-1-1">Ridge Regression vs. Lasso &#x2013; Sparsity</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
The general formulations for ridge regression and lasso problems are:
</p>
<ul class="org-ul">
<li>RR: \(\beta = \arg\min_{\beta}\sum\left(Y - \beta_0 - \sum\beta_i X_i\right)^2\)
</li>
<li>Lasso:
</li>
</ul>

<p>
However, using Lagrange Multipliers, the problems may be formulated as:
</p>
<ul class="org-ul">
<li>RR: 
</li>
<li>Lasso:
</li>
</ul>

<p>
If we visualize these constraints, it's easy to see that a circle intersects with the error function at a point distinct from the axes &#x2013; the optimum is different than zero. On the other hand, the l1 constraint with its diamond shape contacts the error exactly <i>at</i> the axis! The other axes (coefficients) thus end up as zero.
</p>

<p>
A \(q\) value smaller than 1 would improve sparsity, but the problem is no longer convex and 'easy' to solve.
</p>

<p>
As we increase &lambda;, coefficients might not decrease monotonically &#x2013; as we decrease responsibility in some coefficients, some coefficients are in turn assigned more responsibility.
</p>
</div>
</div>

<div id="outline-container-sec-4-1-2" class="outline-4">
<h4 id="sec-4-1-2">Bayesian priors</h4>
</div>

<div id="outline-container-sec-4-1-3" class="outline-4">
<h4 id="sec-4-1-3"><span class="todo TODO">TODO</span> Elastic Net</h4>
<div class="outline-text-4" id="text-4-1-3">
<p>
When combining ridge regression and lasso we get the Elastic Net method:
</p>

<ul class="org-ul">
<li>\(\beta = \arg\min_{\beta}\)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-1-4" class="outline-4">
<h4 id="sec-4-1-4">Principal Components Regression</h4>
<div class="outline-text-4" id="text-4-1-4">
<p>
Instead of using the original variables for the regression, we reduce the number of variables by projecting onto principal components and using the <i>distances</i> between each sample and PC as the variables for regression.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2">Support Vector Machines</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Classification algorithm for <i>binary</i> classification. It's a generalization of a <i>maximal margin classifier</i>, l
</p>
</div>

<div id="outline-container-sec-4-2-1" class="outline-4">
<h4 id="sec-4-2-1">Maximal Margin Classifier</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
Assumption that data is <i>linearly separable</i>, i.e. there exists a linear decision boundary &#x2013;a hyperplane&#x2013; that separates the two classes.
</p>

<p>
In many cases there are many possible candidate hyperplanes &#x2013; which one do we choose?
</p>
<ul class="org-ul">
<li>Maximize the distance between class samples and the hyperplane! This way, when we perform a classification, there's less of a chance that the new observation will have belonged in the other class rather than the chosen one.
</li>
</ul>

<p>
The "margin" is the smallest distance between <i>any</i> training observation and the hyperplane, such that the "maximal margin" is the sort of "widest road" between the classes.
</p>

<p>
The <i>support vectors</i> are the training samples that are equidistant from the hyperplane &#x2013; changing these will change the hyperplane, but changing anything else won't. They're labeled <i>support</i> as the hyperplanes depends on them and them only.
</p>

<p>
Disadvantages:
</p>
<ul class="org-ul">
<li>Sensitive to individual observations
</li>
<li>May overfit the training data!
</li>
</ul>

<p>
Now, what happens if there's no linear separability?
</p>
</div>
</div>

<div id="outline-container-sec-4-2-2" class="outline-4">
<h4 id="sec-4-2-2">Support Vector Classifier</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
This model allows some samples to be on the 'wrong side' of the margin or hyperplane. 
</p>

<p>
We loosen the constraints to allow some samples to be in the middle of the 'road'. However, we still impose penalties using budgeted 'slack variables' &#x2013; these allow violations of the margin. 
</p>

<p>
A slack variable &epsilon;<sub>i</sub> is such that:
</p>
<ul class="org-ul">
<li>&epsilon;<sub>i</sub> = 0 when datum is in correct side of the hyperplane
</li>
<li>&epsilon;<sub>i</sub> &gt; 0 when datum is on wrong side of the margin
</li>
<li>&epsilon;<sub>i</sub> &gt; 1 when datum is outright misclassified
</li>
</ul>

<p>
Having fewer support vectors implies higher variance &#x2013; once again we must control the tradeoff!
</p>

<p>
However, when data is way non-linear this is still useless!
</p>
</div>
</div>

<div id="outline-container-sec-4-2-3" class="outline-4">
<h4 id="sec-4-2-3">Support Vector Machines</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
The main trick is to increase the dimensionality of the data! Add "proprietary" variables such as polinomial combinations of variables. On these higher-dimensional spaces, the data can be linearly separable!
</p>

<p>
We could define these variables by hand, but they might not always be the best choice, and using exhaustive approaches can quickly make our new dataset hard to handle!
</p>

<p>
SVMs do this more or less automatically &#x2013; they use <i>kernels</i> which implicitly map data into higher-dimensional space and then apply support vector classifiers.
</p>
</div>

<ul class="org-ul"><li><a id="sec-4-2-3-1" name="sec-4-2-3-1"></a>Kernels<br  /><div class="outline-text-5" id="text-4-2-3-1">
<p>
Kernels are generalizations of the inner product &#x2013; they give a measure of similarity between two points.
</p>

<p>
The main advantage is in being able to represent these as a <i>feature map</i> which significantly improves the computability.
</p>
</div>

<ul class="org-ul"><li><a id="sec-4-2-3-1-1" name="sec-4-2-3-1-1"></a>Linear Kernel<br  /><div class="outline-text-6" id="text-4-2-3-1-1">
<p>
\(K(X,X') = <X, X'>\)
Equivalent to just using Support Vector classifiers!
</p>
</div>
</li>

<li><a id="sec-4-2-3-1-2" name="sec-4-2-3-1-2"></a>Polynomial Kernel<br  /><div class="outline-text-6" id="text-4-2-3-1-2">
<p>
$K(X, X') = (1 + &lt;X, X'&gt;) $
</p>
</div>
</li>

<li><a id="sec-4-2-3-1-3" name="sec-4-2-3-1-3"></a>Radial Basis Kernel<br  /><div class="outline-text-6" id="text-4-2-3-1-3">
<p>
$K(X,X') = exp (- &gamma; ||X - X'||<sup>2</sup>) $
</p>
</div>
</li></ul>
</li>

<li><a id="sec-4-2-3-2" name="sec-4-2-3-2"></a>Advantages<br  /><div class="outline-text-5" id="text-4-2-3-2">
<ul class="org-ul">
<li>Regularization parameter C to avoid overfitting
</li>
<li>Kernels give flexibility in the shape of the decision boundary
</li>
<li>Convex optimization problem &#x2013; unique solution
</li>
</ul>
</div>
</li>
<li><a id="sec-4-2-3-3" name="sec-4-2-3-3"></a>Disadvantages<br  /><div class="outline-text-5" id="text-4-2-3-3">
<ul class="org-ul">
<li>Must tune hyperparameters &#x2013; poor performance if not well chosen
</li>
<li>Hard to interpret
</li>
<li>Problems <i>must</i> be formulated as binary classification
</li>
</ul>
</div>
</li></ul>
</div>
<div id="outline-container-sec-4-2-4" class="outline-4">
<h4 id="sec-4-2-4">SVMs for more classes?</h4>
<div class="outline-text-4" id="text-4-2-4">
<p>
Two main approaches
</p>
</div>
<ul class="org-ul"><li><a id="sec-4-2-4-1" name="sec-4-2-4-1"></a>One-versus-one<br  /><div class="outline-text-5" id="text-4-2-4-1">
<p>
Classifies every pair of classes
</p>
</div>
</li>
<li><a id="sec-4-2-4-2" name="sec-4-2-4-2"></a>One-versus-all<br  /></li></ul>
</div>
</div>

<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3">Imputation on Missing Data</h3>
<div class="outline-text-3" id="text-4-3">
</div><div id="outline-container-sec-4-3-1" class="outline-4">
<h4 id="sec-4-3-1">Imputation</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
If we threw away every survey that was missing some data, we'd end up throwing away most of our dataset! <i>Imputation</i> is the process of filling in missing data.
</p>

<p>
<i>Inferring computation</i>.
</p>

<p>
Oftentimes, imputation is part of the data wrangling preprocess, in the way of some further analysis, However, sometimes imputation is the whole point! <i>Recommender</i> systems are purely imputation systems usually based in matrix completion.
</p>
</div>

<ul class="org-ul"><li><a id="sec-4-3-1-1" name="sec-4-3-1-1"></a>Missing Completely At Random (MCAR)<br  /><div class="outline-text-5" id="text-4-3-1-1">
<p>
Data might be missing <i>because</i> it doesn't fit some pattern &#x2013; we must ensure that the data that's missing is missing purely randomly.
</p>

<p>
For example, voting turnout data might be linked to geographies &#x2013; If we know some geographical region has certain tendencies and yet it has low turnout in some election, it's not valid to assume that missing votes are MCAR.
</p>

<p>
If we're able to build a classifier that can predict whether data is missing, then the data is <b>not</b> missing at random!
</p>
</div>
</li>

<li><a id="sec-4-3-1-2" name="sec-4-3-1-2"></a>Methods<br  /><ul class="org-ul"><li><a id="sec-4-3-1-2-1" name="sec-4-3-1-2-1"></a>Deleting incomplete observations<br  /><div class="outline-text-6" id="text-4-3-1-2-1">
<p>
Not good &#x2013; we end up losing most of our data!
</p>
</div>
</li>
<li><a id="sec-4-3-1-2-2" name="sec-4-3-1-2-2"></a>Filling in with the mean or median of the data<br  /></li>

<li><a id="sec-4-3-1-2-3" name="sec-4-3-1-2-3"></a>KNN imputation<br  /><div class="outline-text-6" id="text-4-3-1-2-3">
<p>
<code>impute</code> command in R
</p>

<ol class="org-ol">
<li>Fill-in missing values with mean or median for those variables
</li>
<li>Compute the distance between the <i>observation</i> missing a value and all others to find the k closest observations (using <i>all</i> variables)
</li>
<li>Make sure you ignore the variable that's missing the value when computing distances.
</li>
<li>Use the k neighbors to compute a new mean or median
</li>
</ol>

<p>
The reason we first fill-in the missing values is such that we're able to compute distances between observations that may have values missing for variables <i>other</i> than the one we're trying to fill in.
</p>
</div>
</li>
<li><a id="sec-4-3-1-2-4" name="sec-4-3-1-2-4"></a>SVD imputation<br  /><div class="outline-text-6" id="text-4-3-1-2-4">
<ol class="org-ol">
<li>Initalize missing data however
</li>
<li>Use a rank-k SVD to down-dimension and plug in the missing data with the projected value
</li>
<li>Repeat until convergence
</li>
</ol>
</div>
</li></ul>
</li>
<li><a id="sec-4-3-1-3" name="sec-4-3-1-3"></a>Performance on highly sparse datasets<br  /><div class="outline-text-5" id="text-4-3-1-3">
<p>
How do we determine how well we do on the imputation?
</p>

<p>
Similarly with the gap statistic in K-means, we cross-validate with random data as we compare models.
</p>

<p>
With imputation, we need <i>high bias</i>, as there's such a lack of data that high variance is bound to overfit.
</p>
</div>
</li></ul>
</div>
</div>
<div id="outline-container-sec-4-4" class="outline-3">
<h3 id="sec-4-4">Decision Trees &#x2013; Classification and Regression Trees (CART)</h3>
<div class="outline-text-3" id="text-4-4">
<p>
<i>Recursive binary</i> classifier. We continuously subdivide a region based on yes or no conditions, which results in <i>highly interpretable</i> representations of the data.
</p>

<p>
A typical decision function is of the form \(f(x) = \sum(c_m I(x\in R_m))\), where \(I\) is the indicator function, equal to 1 if the condition is fulfilled and 0 if not. The different values of \(c_m\) code the result for particular combinations of regions.
</p>

<p>
In typically decision trees, the branch length encodes the importance of a particular categorization.
</p>
</div>
<div id="outline-container-sec-4-4-1" class="outline-4">
<h4 id="sec-4-4-1">Creation algorithms</h4>
<div class="outline-text-4" id="text-4-4-1">
</div><ul class="org-ul"><li><a id="sec-4-4-1-1" name="sec-4-4-1-1"></a>Greedy Algorithm<br  /><div class="outline-text-5" id="text-4-4-1-1">
<p>
Start with a single split and optimize the one variable and splitting point that minimizes the prediction error. Now consider each subtree as its own tree and repeat!
</p>

<p>
If our variable is categorical, we can <i>aggregate</i> their underlying variables and order its values.
</p>
</div>
</li></ul>
</div>

<div id="outline-container-sec-4-4-2" class="outline-4">
<h4 id="sec-4-4-2">Depth of the tree?</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
Splitting exhaustively is a bias-variance issue &#x2013; if the last category is the training sample itself, the prediction is perfect and we have overfitting!
</p>
</div>

<ul class="org-ul"><li><a id="sec-4-4-2-1" name="sec-4-4-2-1"></a>Stopping Criteria<br  /><ul class="org-ul"><li><a id="sec-4-4-2-1-1" name="sec-4-4-2-1-1"></a>Minimum decrease in error &#x2013; stop on short branches<br  /><div class="outline-text-6" id="text-4-4-2-1-1">
<p>
Not good since an initial poor prediction can enable a much better prediction, but we wouldn't know since we stopped expanding.
</p>
</div>
</li>

<li><a id="sec-4-4-2-1-2" name="sec-4-4-2-1-2"></a>Pruning<br  /><div class="outline-text-6" id="text-4-4-2-1-2">
<p>
We initially grow out the tree completely and later prune the insignificant splits until some desired complexity.
</p>
</div>
</li></ul>
</li></ul>
</div>

<div id="outline-container-sec-4-4-3" class="outline-4">
<h4 id="sec-4-4-3">Advantages</h4>
<div class="outline-text-4" id="text-4-4-3">
<ol class="org-ol">
<li>Handles missing data through surrogate splits
</li>
<li>Robust to non-informative data
</li>
<li>Automatic variable selection &#x2013; no regularization needed!
</li>
<li>Very, very interpretable
</li>
<li>Captures high order interactions
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-4-4-4" class="outline-4">
<h4 id="sec-4-4-4">Disadvantages</h4>
<div class="outline-text-4" id="text-4-4-4">
<ol class="org-ol">
<li>Instability of trees
<ul class="org-ul">
<li>Solution: Random Forests
</li>
</ul>
</li>
<li>Lack of smoothness!
<ul class="org-ul">
<li>Solution: MARS
</li>
</ul>
</li>
<li>Hard to capture additivity
<ul class="org-ul">
<li>Solution: MARS or MART
</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-sec-4-5" class="outline-3">
<h3 id="sec-4-5">Ensemble Methods</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Condorcet's Jury Theorem:
</p>
<ul class="org-ul">
<li>If the probability of a single individual being 'correct' is greather than 0.5, the aggregation of many individuals makes the total jury decision approach probability 1.
</li>
</ul>

<p>
If some machine learning model has a higher-than-random-chance capability of prediction, using <i>many</i> of them will always improve our predictions!
</p>

<p>
Averaging reduces variance but it doesn't increase bias!
</p>

<p>
However, there's a danger in that correlated classifiers don't help as much &#x2013; if the NYT tells people to vote for someone, we no longer have N independent, informed voters; we now have just one.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Day 4</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1">Random Forest and other Ensemble methods</h3>
<div class="outline-text-3" id="text-5-1">
</div><div id="outline-container-sec-5-1-1" class="outline-4">
<h4 id="sec-5-1-1">Monte Carlo methods</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
Many problems do not have closed form solutions, e.g. the mean of the sum of two Negative-Binomial distributed random variables.
</p>
</div>
</div>
<div id="outline-container-sec-5-1-2" class="outline-4">
<h4 id="sec-5-1-2">Bootstrap</h4>
<div class="outline-text-4" id="text-5-1-2">
<p>
It's physically imposible for us to pull ourselves up from our bootstraps &#x2013; however, it's viable to use the same data to generate more data! Useful when we don't have enough data for Monte Carlo methods!
</p>

<p>
Sample from distributions generated from known data, i.e. sample from the <i>histograms</i>! This isn't any more complicated than simply sampling from the dataset. However, we must take care to put our samples <i>back</i> into the original set else we change the shape of the distribution. <span class="underline">Sampling with replacement</span>.
</p>

<p>
It doesn't matter that we end up with duplicates in the sampled dataset.
</p>
</div>
</div>
<div id="outline-container-sec-5-1-3" class="outline-4">
<h4 id="sec-5-1-3">Bagging</h4>
<div class="outline-text-4" id="text-5-1-3">
<p>
<i>Bootstrap aggregation</i>
</p>

<p>
Consider CARTS &#x2013; a different subset of the data might result in a widely differing tree from that generated from another subset. They're <i>unstable</i>, even though the predictions aren't that much different.
</p>

<p>
The concept is to generate multiple predictors from a lot of bootstrapped datasets and aggregate their predictions. The reason this works is that erroneous answers are typically <i>equally erroneous</i>, but those answers that are slightly more correct are correct in consensus!
</p>

<p>
A downside of this method is that we lose interpretability. A single tree is easy to understand, but how a thousand trees affect the prediction is quite a bit harder.
</p>
</div>
<ul class="org-ul"><li><a id="sec-5-1-3-1" name="sec-5-1-3-1"></a>Out-of-bag cross validation!<br  /><div class="outline-text-5" id="text-5-1-3-1">
<p>
There are some samples that are not included in the bootstrapped dataset &#x2013;typically 1/3&#x2013; which can be naturally used for cross-validation.
</p>
</div>
</li></ul>
</div>
<div id="outline-container-sec-5-1-4" class="outline-4">
<h4 id="sec-5-1-4">Boosting</h4>
<div class="outline-text-4" id="text-5-1-4">
<p>
Fitting additional predictors to residuals, i.e. the <i>errors</i>, from initial predictions. We adjust predictions using a percentage (decided with a learning rate &lambda;) of some known errors.
</p>

<p>
Parameters:
</p>
<ul class="org-ul">
<li>Tree number
</li>
<li>Tree depth
</li>
<li>&lambda; value
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-5-1-5" class="outline-4">
<h4 id="sec-5-1-5">Random Forests</h4>
<div class="outline-text-4" id="text-5-1-5">
<p>
Bagged trees only have variability in the way that data is selected for the training. Thus, they can still be correlated amongst themselves, limiting their usefulness.
</p>

<p>
<i>Random forests</i> introduce aditional randomness by <b>enforcing</b> the use of different splitting variables by randomly disallowing some percentage (chosen using CV). For example, bagged trees might be constantly misled into choosing a variable like 'age' over some other one, not allowing the model to find some better variable. This is the same idea as the concept of influenced, non-independent voters.
</p>

<p>
Because of this, each <i>individual</i> tree tends to be worse, but the <i>aggregate</i> is far better.
</p>

<p>
We still get OOB Cross-Validation!
</p>
</div>
<ul class="org-ul"><li><a id="sec-5-1-5-1" name="sec-5-1-5-1"></a>Variable importance measurements<br  /><div class="outline-text-5" id="text-5-1-5-1">
<p>
Measure whether some variable is important or not.
</p>
</div>
<ul class="org-ul"><li><a id="sec-5-1-5-1-1" name="sec-5-1-5-1-1"></a>Metric 1<br  /><div class="outline-text-6" id="text-5-1-5-1-1">
<p>
Average the total error decrease after a split involving this variable over all trees.
</p>
</div>
</li>
<li><a id="sec-5-1-5-1-2" name="sec-5-1-5-1-2"></a>Metric 2<br  /><div class="outline-text-6" id="text-5-1-5-1-2">
<p>
Shuffle <i>only the studied variable</i> and check whether prediction error increases. If it does, it turns out the variable was important!
</p>
</div>
</li></ul>
</li>
<li><a id="sec-5-1-5-2" name="sec-5-1-5-2"></a>Advantages<br  /><div class="outline-text-5" id="text-5-1-5-2">
<ul class="org-ul">
<li>Very robust under non-informative variables &#x2013; less variance
</li>
<li>Less prone to overfitting
</li>
<li>No need for pruning!
</li>
<li>Built-in cross validation sets with OOB
</li>
</ul>
</div>
</li>
<li><a id="sec-5-1-5-3" name="sec-5-1-5-3"></a>DIsadvantages<br  /><div class="outline-text-5" id="text-5-1-5-3">
<ul class="org-ul">
<li>Hard to capture additive effects
</li>
<li>Not interpretable
</li>
</ul>
</div>
</li></ul>
</div>
</div>
<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2">SVM extras</h3>
<div class="outline-text-3" id="text-5-2">
</div><div id="outline-container-sec-5-2-1" class="outline-4">
<h4 id="sec-5-2-1">Imbalanced classes</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
One class occurs far more often than the other, e.g. in fraud detection, medical databses.
</p>

<p>
Problematic because algorithms work best on even databases, and there's typically poor performance on underrepresented the class.
</p>

<p>
How do we imporove performance?
</p>
<ul class="org-ul">
<li>Get more data for underrepresented class!
</li>
<li>Class weighting
<ul class="org-ul">
<li>Penalize more harshly making an error on the smaller class
</li>
</ul>
</li>
<li>Special sampling methods
<ul class="org-ul">
<li>Modify training observations to balance class sizes, tipically by undersampling (+) or oversampling (-). This, however, may throw away important training observations or end up severly overfitting the model.
</li>
</ul>
</li>
</ul>
</div>
<ul class="org-ul"><li><a id="sec-5-2-1-1" name="sec-5-2-1-1"></a>Classification performance<br  /></li></ul>
</div>
</div>
<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3">Machine Learning for Text Data</h3>
<div class="outline-text-3" id="text-5-3">
</div><div id="outline-container-sec-5-3-1" class="outline-4">
<h4 id="sec-5-3-1">Non-negative matrix decomposition</h4>
<div class="outline-text-4" id="text-5-3-1">
<p>
Non-negative matrix factorization \(X=WH,W>=0,H>=0\) found by solving an optimization problem. \(W\) becomes our basis conversion matrix, i.e. our dictionary, and \(H\) contains the coefficients that represent the data.
</p>

<p>
<i>Metafeatures</i>!
</p>
</div>

<ul class="org-ul"><li><a id="sec-5-3-1-1" name="sec-5-3-1-1"></a><span class="todo TODO">TODO</span> Advantages:<br  /><div class="outline-text-5" id="text-5-3-1-1">
<ul class="org-ul">
<li>Interpretability thanks to non-negativity
</li>
</ul>
</div>
</li></ul>
</div>

<div id="outline-container-sec-5-3-2" class="outline-4">
<h4 id="sec-5-3-2">Bag-of-Words model</h4>
<div class="outline-text-4" id="text-5-3-2">
<p>
Only store word frequency information.
</p>
</div>
</div>

<div id="outline-container-sec-5-3-3" class="outline-4">
<h4 id="sec-5-3-3">N-gram</h4>
<div class="outline-text-4" id="text-5-3-3">
<p>
Contiguous sequence of words/characters, frequency counted similar to B.o.W. model.
</p>
</div>
</div>

<div id="outline-container-sec-5-3-4" class="outline-4">
<h4 id="sec-5-3-4">Term-Document Matrix</h4>
<div class="outline-text-4" id="text-5-3-4">
<p>
A term may be either a word or a gram, representing each row in the matrix, whereby every column stores the frequency count for each term in some document
</p>

<p>
Red fish blue fish example
</p>
</div>

<ul class="org-ul"><li><a id="sec-5-3-4-1" name="sec-5-3-4-1"></a><span class="todo TODO">TODO</span> Red fish blue fish example<br  /></li></ul>
</div>
</div>
<div id="outline-container-sec-5-4" class="outline-3">
<h3 id="sec-5-4">Review Questions</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>In highly variable data, it's best to use restrictive models to mitigate overfitting. Only use more flexibility when dealing with data naturally of that kind, i.e. non-linear data.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-5-5" class="outline-3">
<h3 id="sec-5-5">Crash Course on Neural Networks and Deep Learning</h3>
<div class="outline-text-3" id="text-5-5">
</div><div id="outline-container-sec-5-5-1" class="outline-4">
<h4 id="sec-5-5-1">Neural Networks</h4>
<div class="outline-text-4" id="text-5-5-1">
<p>
Neural networks are modeled after physical brain neurons, but only inspirationally. Modern methods have lost most physical analogy.
</p>

<p>
Outputs of nodes are weighted differently as they become the inputs for other nodes.
</p>

<p>
We use non-linear <i>activation functions</i> like ReLU, sigmoids, or hyperbolic tangents, so as to make the behavior of the network more complicated than a simple linear combination of weights.
</p>

<p>
Gradient descent is used to train the weights into minimzing some loss function assosciated with training data and predictions made. By itself, gradient descent is prone to not reaching global minima, though this can be useful for avoiding overfitting.
</p>

<p>
The mathematical theory of neural networks is severly underdeveloped! We use them from a technical perspective without understanding why they perform particularly well on certain datasets.
</p>

<p>
It's a field of tricks!
</p>

<p>
Regularization via <i>early stopping</i> &#x2013; walk towards the minimum but don't actually reach it &#x2013; or <i>dropout</i>, which motivates individual nodes to better respond to structures by themselves, rather than relying on other nodes'  behavior.
</p>
</div>
</div>

<div id="outline-container-sec-5-5-2" class="outline-4">
<h4 id="sec-5-5-2">Deep Learning</h4>
<div class="outline-text-4" id="text-5-5-2">
<p>
Many many layers, many many parameters.
</p>

<p>
Only usable on <i>obscenely</i> large datasets.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2018-01-13 Sat 14:56</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.3.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
