#+TITLE: Machine learning
#+LATEX_CLASS: article
#+LATEX_HEADER: \usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
#+LATEX_HEADER: \setlength{\parindent}{1cm}
#+OPTIONS: num:nil

* Roadmap

#+BEGIN_SRC plantuml file:roadmap.png

if "Do you have labeled data?"
then --> "Supervised Learning"
else --> "Unsupervised Learning"

#+END_SRC

* Day 1

Focus on supervised and unsupervised learning
** Introduction to Machine Learning

*** Available Tools

- R for statistical computing
- Python packages such as scikit-learn or tensorflow
*** Big idea

Find a model f such that for known vectors X and Y, $Y=f(X)+\varepsilon$
*** Model types

**** Parametric Models

We're given a function with a set of parameters, and when training we only have to estimate the parameters instead of an arbitrary function.

Advantages: Easier to estimate
Disadvantages: May not be representative of the real $f$

**** Non-parametric Models

Make no assumptions about $f$ and fit it as closely as possible without overfitting. Example: Spline fitting

Advantages: Captures local features very well
Disadvantages: Risk of overfitting; requires large dataset
*** Accuracy vs Interpretability tradeoff

*** Main Categories of Models

**** Supervised Learning

We have input data and the associated response. We have both $X^(i)$ and $Y^(i)$
***** Regression 

Quantitative output,
***** Classification

      Qualitative (categorical) output. "Whether-or-not" problems. May be formulated as a regression problem linked to /probabilities/ of belonging to some category.
**** Unsupervised Learning

We have the input data but no response data, i.e. only $X^{(i)} not $Y^{(i)}
***** Clustering

      /Partition/ data into subsets that share common characteristics
***** Dimensionality reduction

      Create /new features/ that best characterize high-dimensional data


** Introduction to Unsupervised Learning

Goal: Find patterns/properties, usually for visualizing /and interpreting/ high-D data.

**** Sample applications

- Recovering compact representations (bases) for high dimensional data

**** Challenges

This is exploratory data analysis -- We don't know what the right answer is and our goal is never clearly defined.

*** Clustering

Ways of finding subgroups within the data without any man-made labels.

Careful with confusing it with classification. Classification is the process of /selecting/ a subgroup for some new element, whereas clustering is the process of /finding/ the subgroups.

**** Types of clustering models

- Centroid-based clusters
- Hierarchical

**** Hierarchical clustering

Turns data into tree-like representations organized by the /distance/ between different nodes. By specifying a cut-off depth we may manipulate the resolution of the clusters.

How do you calculate the distance between clusters?
- Single-linkage: Calculate using the closest pair /only/.
- Complete-linkage: Calculate using the furthest pair /only/.
- Average linkage: Calculate using the distance between all combinations of nodes.

**** K-means clustering

Each of $K$ clusters is defined by a /centroid vector/, and each observation is assigned to a single cluster through the nearest centroid. As an input it requires the number $K$ of desired clusters.

Measure of similarity through the Eucledian distance.

Goal is to minimze /within-cluster variation/ $J$ through least-squares optimization.

***** Basics of the algorithm

At each step:
1. Define initial cluster centroids
2. Partition data by assigning /each sample/ to the nearest centroid (2-norm)
3. *Recalculate* the centroids within each partition
Repeat 2&3 until clusters are invariant.
#+BEGIN_SRC python

  while centroid_displacement_from_previous_iteration() > eps:
      (sample.select_nearest_centroid() for sample in samples)
      recompute_centroids()
      

#+END_SRC

***** Initialization

Poor initialization may lead to improper clustering!

Different ways of doing it:
- Random selection of K centroids
- Random partition of data
- Select K points that are /mutually far apart/
- Domain-knowledge
- Initialize using results from some other method
- Average over many clustering runs


***** Cluster number

We may have domain knowledge to inform a proper choice of K, but usually it can be determined from the data itself.

We /can't/ just pick the $K$ that minimizes $J$ since the ideal clustering is then to make /each point its own cluster/.

A heuristic methods runs different K values and plots the corresponding J value. We can look at the "elbow" of the $K$ vs. $J$ plot, at which we don't get any additional benefit by adding more 

***** Advantages

- Easy to implement
- Often converges in very few operations
- Can be applied on data with many features

***** Disadvantages

- We must chose $K$, risking not having an optimal solution or being forced to perform multiple expensive runs.
- Iterative algorithm returns /local minima/.
- By using the 2-norm, it assumes that all clusters are spherical and about the same size.
- Very sensitive to outliers, as with any method that relies on means.

However, there are many variants that fix some of these issues.
- Running algorithm multiple times
- Using medians instead of means
- Require that the centroid /must be a data point/
  - Robust to outliers
  - Flexible -- can use any similarity measure
  - Computationally expensive to calculate the mean

***** Possible modifications

Many possible modifications, like weighing the distance by some metric. For example, if we know a particular subset of a dataset is noisy, we can downplay its importance by weighing its distance to the centroid less than we weigh other subsets.



*** Dimensionality Reduction

For visualization and data intuition purposes it's necessary to wrangle higher dimensions into human-understandable terms. This is basically the science of making good projections onto lower-dimensional spaces. We /always/ lose information.


**** Maximal Variance Projection (PCA)

Usually we'll chose hyperplanes that /retain the spread/ of the data, i.e. maximizing variance. This is *Principal Component Analysis*.

If we have a 2D oval cluster, projecting onto the line that goes through the major axis will maximize the variance. This line is found through either the:
- Sample Covariance matrix: $X^TX=VD^2V^T$
- Singular Value Decomposition: $X=UDV^T$
Where X: Data matrix, V: Eigenvalue matrix, D: Eigenvector matrix

That is, the eigenvector that corresponds to the /largest eigenvalue/ is this best-projection line. These eigenvalues are the variances!

Check out the paper "Genes mirror geography within Europe". Very high-dimensional data, when projected on 2D using PCA, /approximates Europe's geography!/

***** Steps:

- Centering (always) -- Subtract the centroid from each data point to center it at the origin.
- Normalizing (used for correlation PCA; not the same thing)
- Whitening (never) --  Standarize units by dividing by standard deviations.


***** Differences with Linear Regression

A linear regression minimizes the square of the /vertical/ distance between some sample and the regressor. However, the PCA fit minimizes the total /euclidean/ distance between data and fit.


***** Choosing Principal Components

We'll usually choose 2 PCs to visualize data in paper media or presentations.

We can measure how much of the variance is lost. Recall that the eigenvalues capture the variance of each PC eigenvector! If we plot a PC vs. $\sigma^2$ histogram, we can look for steep drop-offs. If we don't find these, there's a lot of information we're losing by reducing dimensionality. In these cases, be careful of finding degenerate eigenvectors (equal eigenvalues) -- both of these must be present, as any linear combination of these is also valid

****** Analytical Method

       Variance explained: $W_k = \frac{1}{N}\sum_{i=1}^{k} {d_i^2}$ (Average square error)


       Gap Statistic: $\log W_k - \log W_k^*$


       $W_k^*$ is the variation found from random data (same N) simulated inside some bounding region (bounding box, convex hull, etc). By plotting the gap statistic, we measure how much /better/ our fit works on the data than with random data. The maximum gives us the optimum k.


       This is also used in k-means with the $J$ metric instead of $W$.


***** Dangers

The first PC is not always the best projection! Imagine a data set with parallel oblique clusters -- the first PC is parallel to these clusters, as it captures the most spread. However, by projecting onto it, the clusters merge and we'll think our data is completely structureless!


**** Self Organizing Maps

If we find a hypercurve that best fits the data, it's possible to describe all the data with a single parameter. However, finding an arbitrary curve is a rather difficult problem.

If we define an initial straight curve (first principal component, usually) defined by N points $m_k$, we iterate by making every sample point $x_i$ /pull/ the closest $m_k$ towards itself, proportional to how far apart they are. This is described by the relation $m_k \leftarrow m_k + \alpha\left(x_i - m_k\right)$. The fact that the pulling action is averaged over all the nearby data points is key for preserving the curve structure.

The parameter \alpha is called the /learning rate/, and is usually best set to a small value to approach the solution smoothly through lots of iterations rather than jerking back and forth and risk accuracy.


**** Multi-Dimensional Scaling (MDS)

By defining distance metrics between variables (can be esoteric), we record these distances in a distance map which we can reconstruct using least-squares minimization. The idea of reconstructing maps from 'road distances', for example, is viable! The resulting map might be arbitrarily rotated, but it will capture geometrical relationships very well.


These are affine spaces in which we can't define an origin but we can define distances.


$\min_{x_1,...,x_N} \sum_{i<j} \left(||x_i-x_j||-\delta_{ij}\right)^2$


**** Independent Component Analysis (ICA)
Best used when data is a linear superposition of signals, like with audio. Let's assume we have three microphones in a room and we want to isolate each independent sound source out of three: TV, Radio, Speech.

We have a data matrix with each variable (microphone) over time which we'll whiten/normalize. For this we compute all PCs, and scale along each of these by the according eigenvalue, after which direcions become uncorrelated but not independent.

From this, we want to the lowest entropy projection, or that which is non-Gaussian. This is, that it has /structure/ as measured by some entropy metric. Projecting on these will ensure that we maintain structure, rather than just variance.

An important limitation is that you can only find as many projections as you have 'measuring devices'.

This has applications in finding patterns in financial data, earth monitoring, etc.

* Day 2
** Measuring algorithm performance
Loss functions quantify cost of errors. E.g: Mean Squared Error (MSE) = $\frac{1}{N}\sum\left(y_i-f(x_i)\right)^2$

Keep in mind that we don't want to minimize the error on the data we have (that leads to overfitting. Rather, we want to minimize the error on /new data/, such that we build a model that /generalizes/, rather than just fit the trends.

In the end, we want to minimize the /expected loss/ on future data.

Training error and Test error behave differently. As our model flexibility increases, the training error always decreases, but past a certain point the test error will actually begin to increase!

When the number of degrees of freedom approaches the size of the data we get into dangerous overfitting territory. Complex models like deep neural networks involve millions of parameters, though at the same time they require far larger datasets to train.
*** Cross Validation
Divide the data into three subsets:
- Training data: Subset used to learn the model
- Validation data: Subset used to estimate error for tuning or model selection
- Test data: Subset used to check model performance. This has not been previously been used on the model.
**** Bias vs. Variance Tradeoff
The expectation of the error for the given estimator (i.e., model) is given by

$\text{Variance(}f\hat\text{)} + \text{Bias(}f\hat\text{)}^2 + \text{Variance(}\varepsilon\text{)}$

The bias represents the ability of the model to represent the actual trend. For example, trying to linearly fit a nonlinear model is biased towards the linear fit wheras a slightly more complex model would fit the data better.

Variance, on the other hand, quantifies how much the estimation varies with different datasets.

As we increase the DOFs, bias is reduced, but at the same time variance is increased.

Ideally, low bias /and/ low variance imply a low test error.
** Classification
*** K-Nearest Neighbor classifier
Basic idea: classify observations based on nearby labels.

The predicted class for a sample X is the most common class among its K nearest neighbors from the training set.

The probability of belonging to some class is quite easily $\frac{\#\text{(class among neighbors)}}{K}$.

Decision boundaries are a useful way of visualizing the regions of classification as well as noting the complexity of some fit.

This is a /non-parametric/ model, as we're not fitting an equation with fixed parameters.

Useful for imputating data!
**** Choosing K
With small K, we overfit and the decision boundary is very rough and jittery. However, with a large K the data might not be abundant enough to capture /any/ trends.

Again, by minimizing the expected error we can choose the optimal K.
**** Advantages
- Simple to implement
- Few tuning parameters (K, distance metric)
- Flexible, doesn't impose linear separability
**** Disadvantages
- Computationally expensive
- Sensitive to imbalanced datasets (larger classes smother the smaller ones)
- Sensitive to irrelevant inputs
*** Regression
**** Linear Regression
It's a simple supervised learning method! Its advantage is /highly interpretable/ as the slope parameters easily quantify the impact of individual variables.
***** Simple Linear Regression
Parametric model given by $Y=\beta_0 + \beta_1 X + \varepsilon$.

We estimate $\hat\beta_0 and $\hat\beta_1 using training data to find a good fit, and for this we usually use the Mean Squared Error.

SLR in particular has a closed-form solution for the parameters that gives the best least-squares fit.
***** Multiple Linear Regression
We use more than one predictor (X) variable on the form Y=\beta_0+\beta_1 X+\beta_2 X_2+...

Using the magic of linear algebra, we may define X and \beta vectors to set up the optimization problem \hat\beta = \arg\min_{\beta} ||Y - X^{T}\beta ||^2. This also has a closed-form solution (found through the /normal equations/) given by $\beta = \left(X^TX\right)^{-1}X^TY$

***** Variations on Linear Regression
****** Weighted Linear Regression
Not all data is created equal -- if we know a particular subset of data is less reliable, i.e. more noisy, we reduce its impact on the model through weighting.

Using a diagonal matrix $W$ with elements $w_i = \frac{1}{\sigma_i^2} where each \sigma_i may be esoteric or properly measured. With this, the problem becomes
****** Locally weighted linear regression
We pressume an /a priori/ interest in some particular region of data. For some observation, we apply a gaussian weighing surrounding the point of interest to produce a sort of tangent line.

A new linear fit must be produced for /every/ desired observation, and in the end this produces a piece-wise linear approximation to the trend curve.
**** Logistic Regression
***** Qualitative inputs
Class predictors like KNN are based on purely categorical data. However, it's possible to classify /using regression/ by regressing probabilities of belonging to different classes.

When input data is qualitative, (e.g. "female", "male", "blue eyes", "brown eyes") it's easy to use binary variables that quantify this. However, it's dangerous to apply a linear regression with categories assigned to different numbers (i.e. symptoms: headache:1, seizure:2, stroke:3) to any categories as this introduces unnatural orderings that bias the fit.

The best way of dealing with categorical data is using /binary variables/. Either 1 or 0 whether the category is true or not.
***** Description
$Y$ takes on two values: 0 or 1, and we estimate it with probabilities in the interval $[0-1]$. 

We use the sigmoid/logistic function $\sigma(z)=\frac{1}{1+e^{-z}}$ that lies in the desired interval. By making $z$ the linear regressor (Xs aren't forced to be binary), we can "squash" the data to perform categorical fitting.

By choosing the sub-space where $\mathbb{P}=p$ we can define the decision boundary for the classification, so instead of fitting a line, we're fitting a sort of /step/ function.
***** Advantages
- Extension of Linear Regression
- Interpretability -- log-odds are linear
- No tuning of hyperparameters
***** Disadvantages
- Can't model complex decision boundaries
- May overfit in training data, though it can be mitigated with Regularization in the MLE method
- Problem *must* be formulated as binary classification

** Cross-Validation and Regularization

*** Cross-Validation

**** XKCD example with the jelly beans and medical research
[[https://xkcd.com/882/][Statistical significance]] has no meaning if we don't properly manage our p-values to account for the fact that we're bombarding the data with the same models. If we get results, it's probably because we arrived at the $p%$ of times that we get it from pure chance!
**** TODO Loss Functions
In linear regression, why do we use the squared error instead of just the absolute error? Usually it's because it gives us a closed form solution.

***** Squared Error
$\sum_j\left(Y-f(X)\right)^2$

This is affected much more by outliers, as they make the square error far larger than the simple distance. This error function leads to the mean, which presents high sensitivity.
***** Absolute Error
$\sum_j\left|Y-f(X)\right|$

Since there's no further penalization of far-away terms, this loss function is less sensitive to outliers. This gives rise to the median, a far more stable central measure.

**** TODO Classification Loss Functions
Indicator error (nuumber of times "I screwed up", i.e. classifying spam(1) as not-spam(0)).


"Flip" what we're saying: If we get something wrong 80% of the time, just flip the classification!

***** TODO Misclassification

Gini index of uniformity

**** TODO Supervised Learning Theory

Given a loss function $L(Y, \tilde f(X))$, our goal is to find $\hat f = \argmin_{\tilde f} \mathbb{E}[L(Y, \tilde f(X))]$. That is, across the population $X$, find the model $\tilde f$ that minimizes the expected error.

**** K-fold Cross Validation
We divide our data into different subsets, some of which are used for testing and some of which are used to /validate/ the model. We can reshuffle the dataset and do this multiple times to get an average.

Cross-Validation Error: (tests whether a */type/* of model is a good fit, rather than a particular fit). This cannot be used to replace the necessity for validation sets!

N-fold cross-validation is the best validation method -- it leaves one sample out and uses everybody else to train and later validate one the one sample. It performs this N times, meaning it's very expensive for more complex models.

Good compromises are 5-fold and 10-fold cross valdiations.

/Types/ of models can be understood as different parameters for a single model, e.g. 2 neighbors for KNN vs 3 neighbors, etc.

**** Bias-Variance tradeoff
     See [[Bias vs. Variance Tradeoff][Previous Heading]].

**** Learning Curves
How do we answer the question: do we need a better model or do we need more data?

Compare the training error and Cross-validation error as the data size changes /with the same model/. As we increase the data size, the same fit isn't as able to fit newer data so the /training error increases/ as the cross-validation error decreases and eventually bottoms out. These errors never actually cross, but when the curves flatten out it's time to get a better model.

In summary, big gap implies we need more data, small gaps imply it's time to improve the model.

**** Common problems!!
The test sets must /never be used until testing/. We can't use them to extract information that will affect how we develop the models.

Be careful with using too many different models on the same test set. Eventually, a model will /by chance/ fit the model well.

**** TODO Importance of the validation set
Only, and only when we've exhausted the model space and found a model that might

**** Training data issues

***** Data is biased!
As an example, historical text incorporates sexism (4 times more male references to female ones, as well as other biases in content) that gets incorporated into automated systems unnoticed.

#+BEGIN_QUOTE
Machine Learning is like money laundering for bias. It's a clean, mathematical apparatus that gives the status quo the aura of logical invenitability
--Maciej Ceglowski
#+END_QUOTE

*** Regularization
"Too many cooks spoil the soup"

Many models for regression involve many variables or so-called "proprietary" variables -- functions of the other variables.

**** Problems with models
***** More variables than samples
Having too few samples for very high-dimensional models allow us to fit an infinitude of spaces. For example fitting two samples to a three dimensional model lets us fit infinitely many planes that pass through the line connecting the samples.
***** Multiple-Colinearity
What can go wrong when we have a few variables that are highly correlated between each other but not with the variable we're trying to predict? When performing a fit, these variables won't affect the predictor and might happen to cancel out mutually -- because of this, their weights might become disproportionally large, making it seem like these variables are actually very important when in fact they're not!


**** Ridge Regression
Incorporate a 'penalized error' $\lambda \left(\sum\beta_i^2\right)$. 

As \lambda increases, the /bias/ increases as we end up simplifying the model (\beta_i tend to zero), and the variance decreases as we always reach the same estimations.

**** Lasso
How do we choose which variables can be tuned to /exactly/ zero?

"Least Absolute Shrinkage and Selection Operator"

Rather than squaring the betas, we use their absolute value. Thus, the penalty is $\lambda\left(\sum|\beta_i|\right)$

This becomes an L1 problem rather than an L2 problem used in Ridge Regression. It doesn't find smooth values very well but it approaches zero very reliably. Because of this, our data is able to become sparse much more reliably.

* Day 3

** Back to Regularization
*** Ridge Regression vs. Lasso -- Sparsity 
The general formulations for ridge regression and lasso problems are:
- RR: $\beta = \arg\min_{\beta}\sum\left(Y - \beta_0 - \sum\beta_i X_i\right)^2$
- Lasso:

However, using Lagrange Multipliers, the problems may be formulated as:
- RR: 
- Lasso:

If we visualize these constraints, it's easy to see that a circle intersects with the error function at a point distinct from the axes -- the optimum is different than zero. On the other hand, the l1 constraint with its diamond shape contacts the error exactly /at/ the axis! The other axes (coefficients) thus end up as zero.

A $q$ value smaller than 1 would improve sparsity, but the problem is no longer convex and 'easy' to solve.

As we increase \lambda, coefficients might not decrease monotonically -- as we decrease responsibility in some coefficients, some coefficients are in turn assigned more responsibility.

*** Bayesian priors

*** TODO Elastic Net
    When combining ridge regression and lasso we get the Elastic Net method:

- $\beta = \arg\min_{\beta}$

*** Principal Components Regression
Instead of using the original variables for the regression, we reduce the number of variables by projecting onto principal components and using the /distances/ between each sample and PC as the variables for regression.

** Support Vector Machines
Classification algorithm for /binary/ classification. It's a generalization of a /maximal margin classifier/, l

*** Maximal Margin Classifier
Assumption that data is /linearly separable/, i.e. there exists a linear decision boundary --a hyperplane-- that separates the two classes.

In many cases there are many possible candidate hyperplanes -- which one do we choose?
- Maximize the distance between class samples and the hyperplane! This way, when we perform a classification, there's less of a chance that the new observation will have belonged in the other class rather than the chosen one.

The "margin" is the smallest distance between /any/ training observation and the hyperplane, such that the "maximal margin" is the sort of "widest road" between the classes.

The /support vectors/ are the training samples that are equidistant from the hyperplane -- changing these will change the hyperplane, but changing anything else won't. They're labeled /support/ as the hyperplanes depends on them and them only.

Disadvantages:
- Sensitive to individual observations
- May overfit the training data!

Now, what happens if there's no linear separability?

*** Support Vector Classifier
This model allows some samples to be on the 'wrong side' of the margin or hyperplane. 

We loosen the constraints to allow some samples to be in the middle of the 'road'. However, we still impose penalties using budgeted 'slack variables' -- these allow violations of the margin. 

A slack variable \varepsilon_i is such that:
- \varepsilon_i = 0 when datum is in correct side of the hyperplane
- \varepsilon_i > 0 when datum is on wrong side of the margin
- \varepsilon_i > 1 when datum is outright misclassified

Having fewer support vectors implies higher variance -- once again we must control the tradeoff!

However, when data is way non-linear this is still useless!

*** Support Vector Machines
The main trick is to increase the dimensionality of the data! Add "proprietary" variables such as polinomial combinations of variables. On these higher-dimensional spaces, the data can be linearly separable!

We could define these variables by hand, but they might not always be the best choice, and using exhaustive approaches can quickly make our new dataset hard to handle!

SVMs do this more or less automatically -- they use /kernels/ which implicitly map data into higher-dimensional space and then apply support vector classifiers.

**** Kernels
Kernels are generalizations of the inner product -- they give a measure of similarity between two points.

The main advantage is in being able to represent these as a /feature map/ which significantly improves the computability.

***** Linear Kernel
$K(X,X') = <X, X'>$
Equivalent to just using Support Vector classifiers!

***** Polynomial Kernel
$K(X, X') = (1 + <X, X'>) $

***** Radial Basis Kernel
$K(X,X') = \exp (- \gamma ||X - X'||^2) $

**** Advantages
- Regularization parameter C to avoid overfitting
- Kernels give flexibility in the shape of the decision boundary
- Convex optimization problem -- unique solution
**** Disadvantages
- Must tune hyperparameters -- poor performance if not well chosen
- Hard to interpret
- Problems /must/ be formulated as binary classification
*** SVMs for more classes?
Two main approaches
**** One-versus-one
Classifies every pair of classes
**** One-versus-all

** Imputation on Missing Data

*** Imputation
If we threw away every survey that was missing some data, we'd end up throwing away most of our dataset! /Imputation/ is the process of filling in missing data.

/Inferring computation/.

Oftentimes, imputation is part of the data wrangling preprocess, in the way of some further analysis, However, sometimes imputation is the whole point! /Recommender/ systems are purely imputation systems usually based in matrix completion.

**** Missing Completely At Random (MCAR)
Data might be missing /because/ it doesn't fit some pattern -- we must ensure that the data that's missing is missing purely randomly.

For example, voting turnout data might be linked to geographies -- If we know some geographical region has certain tendencies and yet it has low turnout in some election, it's not valid to assume that missing votes are MCAR.

If we're able to build a classifier that can predict whether data is missing, then the data is *not* missing at random!

**** Methods
***** Deleting incomplete observations
Not good -- we end up losing most of our data!
***** Filling in with the mean or median of the data

***** KNN imputation
=impute= command in R

1. Fill-in missing values with mean or median for those variables
2. Compute the distance between the /observation/ missing a value and all others to find the k closest observations (using /all/ variables)
3. Make sure you ignore the variable that's missing the value when computing distances.
4. Use the k neighbors to compute a new mean or median

The reason we first fill-in the missing values is such that we're able to compute distances between observations that may have values missing for variables /other/ than the one we're trying to fill in.
***** SVD imputation
1. Initalize missing data however
2. Use a rank-k SVD to down-dimension and plug in the missing data with the projected value
3. Repeat until convergence
**** Performance on highly sparse datasets
How do we determine how well we do on the imputation?

Similarly with the gap statistic in K-means, we cross-validate with random data as we compare models.

With imputation, we need /high bias/, as there's such a lack of data that high variance is bound to overfit.
** Decision Trees -- Classification and Regression Trees (CART)
/Recursive binary/ classifier. We continuously subdivide a region based on yes or no conditions, which results in /highly interpretable/ representations of the data.

A typical decision function is of the form $f(x) = \sum(c_m I(x\in R_m))$, where $I$ is the indicator function, equal to 1 if the condition is fulfilled and 0 if not. The different values of $c_m$ code the result for particular combinations of regions.

In typically decision trees, the branch length encodes the importance of a particular categorization.
*** Creation algorithms
**** Greedy Algorithm
Start with a single split and optimize the one variable and splitting point that minimizes the prediction error. Now consider each subtree as its own tree and repeat!

If our variable is categorical, we can /aggregate/ their underlying variables and order its values.

*** Depth of the tree?
Splitting exhaustively is a bias-variance issue -- if the last category is the training sample itself, the prediction is perfect and we have overfitting!

**** Stopping Criteria

***** Minimum decrease in error -- stop on short branches
Not good since an initial poor prediction can enable a much better prediction, but we wouldn't know since we stopped expanding.

***** Pruning
We initially grow out the tree completely and later prune the insignificant splits until some desired complexity.

*** Advantages
1. Handles missing data through surrogate splits
2. Robust to non-informative data
3. Automatic variable selection -- no regularization needed!
4. Very, very interpretable
5. Captures high order interactions

*** Disadvantages
1. Instability of trees
   - Solution: Random Forests
2. Lack of smoothness!
   - Solution: MARS
3. Hard to capture additivity
   - Solution: MARS or MART
** Ensemble Methods
Condorcet's Jury Theorem:
- If the probability of a single individual being 'correct' is greather than 0.5, the aggregation of many individuals makes the total jury decision approach probability 1.

If some machine learning model has a higher-than-random-chance capability of prediction, using /many/ of them will always improve our predictions!

Averaging reduces variance but it doesn't increase bias!

However, there's a danger in that correlated classifiers don't help as much -- if the NYT tells people to vote for someone, we no longer have N independent, informed voters; we now have just one.

* Day 4
** Random Forest and other Ensemble methods 
*** Monte Carlo methods
Many problems do not have closed form solutions, e.g. the mean of the sum of two Negative-Binomial distributed random variables.
*** Bootstrap
It's physically imposible for us to pull ourselves up from our bootstraps -- however, it's viable to use the same data to generate more data! Useful when we don't have enough data for Monte Carlo methods!

Sample from distributions generated from known data, i.e. sample from the /histograms/! This isn't any more complicated than simply sampling from the dataset. However, we must take care to put our samples /back/ into the original set else we change the shape of the distribution. _Sampling with replacement_.

It doesn't matter that we end up with duplicates in the sampled dataset.
*** Bagging
/Bootstrap aggregation/

Consider CARTS -- a different subset of the data might result in a widely differing tree from that generated from another subset. They're /unstable/, even though the predictions aren't that much different.

The concept is to generate multiple predictors from a lot of bootstrapped datasets and aggregate their predictions. The reason this works is that erroneous answers are typically /equally erroneous/, but those answers that are slightly more correct are correct in consensus!

A downside of this method is that we lose interpretability. A single tree is easy to understand, but how a thousand trees affect the prediction is quite a bit harder.
**** Out-of-bag cross validation!
There are some samples that are not included in the bootstrapped dataset --typically 1/3-- which can be naturally used for cross-validation.
*** Boosting
    Fitting additional predictors to residuals, i.e. the /errors/, from initial predictions. We adjust predictions using a percentage (decided with a learning rate \lambda) of some known errors.

Parameters:
- Tree number
- Tree depth
- \lambda value
*** Random Forests
Bagged trees only have variability in the way that data is selected for the training. Thus, they can still be correlated amongst themselves, limiting their usefulness.

/Random forests/ introduce aditional randomness by *enforcing* the use of different splitting variables by randomly disallowing some percentage (chosen using CV). For example, bagged trees might be constantly misled into choosing a variable like 'age' over some other one, not allowing the model to find some better variable. This is the same idea as the concept of influenced, non-independent voters.

Because of this, each /individual/ tree tends to be worse, but the /aggregate/ is far better.

We still get OOB Cross-Validation!
**** Variable importance measurements
Measure whether some variable is important or not.
***** Metric 1
Average the total error decrease after a split involving this variable over all trees.
***** Metric 2
Shuffle /only the studied variable/ and check whether prediction error increases. If it does, it turns out the variable was important!
**** Advantages
- Very robust under non-informative variables -- less variance
- Less prone to overfitting
- No need for pruning!
- Built-in cross validation sets with OOB
**** DIsadvantages
- Hard to capture additive effects
- Not interpretable
** SVM extras
*** Imbalanced classes
One class occurs far more often than the other, e.g. in fraud detection, medical databses.

Problematic because algorithms work best on even databases, and there's typically poor performance on underrepresented the class.

How do we imporove performance?
- Get more data for underrepresented class!
- Class weighting
  - Penalize more harshly making an error on the smaller class
- Special sampling methods
  - Modify training observations to balance class sizes, tipically by undersampling (+) or oversampling (-). This, however, may throw away important training observations or end up severly overfitting the model.
**** Classification performance
** Machine Learning for Text Data
*** Non-negative matrix decomposition
Non-negative matrix factorization $X=WH,W>=0,H>=0$ found by solving an optimization problem. $W$ becomes our basis conversion matrix, i.e. our dictionary, and $H$ contains the coefficients that represent the data.

/Metafeatures/!

**** TODO Advantages:
- Interpretability thanks to non-negativity

*** Bag-of-Words model
Only store word frequency information.

*** N-gram
Contiguous sequence of words/characters, frequency counted similar to B.o.W. model.

*** Term-Document Matrix
A term may be either a word or a gram, representing each row in the matrix, whereby every column stores the frequency count for each term in some document

Red fish blue fish example

**** TODO Red fish blue fish example
** Review Questions
- In highly variable data, it's best to use restrictive models to mitigate overfitting. Only use more flexibility when dealing with data naturally of that kind, i.e. non-linear data.
** Crash Course on Neural Networks and Deep Learning
*** Neural Networks
Neural networks are modeled after physical brain neurons, but only inspirationally. Modern methods have lost most physical analogy.

Outputs of nodes are weighted differently as they become the inputs for other nodes.

We use non-linear /activation functions/ like ReLU, sigmoids, or hyperbolic tangents, so as to make the behavior of the network more complicated than a simple linear combination of weights.

Gradient descent is used to train the weights into minimzing some loss function assosciated with training data and predictions made. By itself, gradient descent is prone to not reaching global minima, though this can be useful for avoiding overfitting.

The mathematical theory of neural networks is severly underdeveloped! We use them from a technical perspective without understanding why they perform particularly well on certain datasets.

It's a field of tricks!

Regularization via /early stopping/ -- walk towards the minimum but don't actually reach it -- or /dropout/, which motivates individual nodes to better respond to structures by themselves, rather than relying on other nodes'  behavior.

*** Deep Learning
Many many layers, many many parameters.

Only usable on /obscenely/ large datasets.
