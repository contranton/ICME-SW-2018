% Created 2018-01-12 Fri 12:19
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\setlength{\parindent}{1cm}
\date{\today}
\title{Machine learning}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.1.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents


\section*{Roadmap}
\label{sec-1}

\begin{verbatim}
if "Do you have labeled data?"
then --> "Supervised Learning"
else --> "Unsupervised Learning"
\end{verbatim}

\section*{Day 1}
\label{sec-2}

Focus on supervised and unsupervised learning
\subsection{Introduction to Machine Learning}
\label{sec-2-1}

\subsubsection*{Available Tools}
\label{sec-2-1-1}

\begin{itemize}
\item R for statistical computing
\item Python packages such as scikit-learn or tensorflow
\end{itemize}
\subsubsection*{Big idea}
\label{sec-2-1-2}

Find a model f such that for known vectors X and Y, $Y=f(X)+\varepsilon$
\subsubsection*{Model types}
\label{sec-2-1-3}

\begin{itemize}
\item Parametric Models
\label{sec-2-1-3-1}

We're given a function with a set of parameters, and when training we only have to estimate the parameters instead of an arbitrary function.

Advantages: Easier to estimate
Disadvantages: May not be representative of the real $f$

\item Non-parametric Models
\label{sec-2-1-3-2}

Make no assumptions about $f$ and fit it as closely as possible without overfitting. Example: Spline fitting

Advantages: Captures local features very well
Disadvantages: Risk of overfitting; requires large dataset
\end{itemize}
\subsubsection*{Accuracy vs Interpretability tradeoff}
\label{sec-2-1-4}

\subsubsection*{Main Categories of Models}
\label{sec-2-1-5}

\begin{itemize}
\item Supervised Learning
\label{sec-2-1-5-1}

We have input data and the associated response. We have both $X^(i)$ and $Y^(i)$
\begin{itemize}
\item Regression
\label{sec-2-1-5-1-1}

Quantitative output,
\item Classification
\label{sec-2-1-5-1-2}

Qualitative (categorical) output. "Whether-or-not" problems. May be formulated as a regression problem linked to \emph{probabilities} of belonging to some category.
\end{itemize}
\item Unsupervised Learning
\label{sec-2-1-5-2}

We have the input data but no response data, i.e. only \$X$^{\text{(i)}}$ not \$Y$^{\text{(i)}}$
\begin{itemize}
\item Clustering
\label{sec-2-1-5-2-1}

\emph{Partition} data into subsets that share common characteristics
\item Dimensionality reduction
\label{sec-2-1-5-2-2}

Create \emph{new features} that best characterize high-dimensional data
\end{itemize}
\end{itemize}


\subsection{Introduction to Unsupervised Learning}
\label{sec-2-2}

Goal: Find patterns/properties, usually for visualizing \emph{and interpreting} high-D data.

\begin{itemize}
\item Sample applications
\label{sec-2-2-0-1}

\begin{itemize}
\item Recovering compact representations (bases) for high dimensional data
\end{itemize}

\item Challenges
\label{sec-2-2-0-2}

This is exploratory data analysis -- We don't know what the right answer is and our goal is never clearly defined.

\subsubsection*{Clustering}
\label{sec-2-2-1}

Ways of finding subgroups within the data without any man-made labels.

Careful with confusing it with classification. Classification is the process of \emph{selecting} a subgroup for some new element, whereas clustering is the process of \emph{finding} the subgroups.

\begin{itemize}
\item Types of clustering models
\label{sec-2-2-1-1}

\begin{itemize}
\item Centroid-based clusters
\item Hierarchical
\end{itemize}

\item Hierarchical clustering
\label{sec-2-2-1-2}

Turns data into tree-like representations organized by the \emph{distance} between different nodes. By specifying a cut-off depth we may manipulate the resolution of the clusters.

How do you calculate the distance between clusters?
\begin{itemize}
\item Single-linkage: Calculate using the closest pair \emph{only}.
\item Complete-linkage: Calculate using the furthest pair \emph{only}.
\item Average linkage: Calculate using the distance between all combinations of nodes.
\end{itemize}

\item K-means clustering
\label{sec-2-2-1-3}

Each of $K$ clusters is defined by a \emph{centroid vector}, and each observation is assigned to a single cluster through the nearest centroid. As an input it requires the number $K$ of desired clusters.

Measure of similarity through the Eucledian distance.

Goal is to minimze \emph{within-cluster variation} $J$ through least-squares optimization.

\begin{itemize}
\item Basics of the algorithm
\label{sec-2-2-1-3-1}

At each step:
\begin{enumerate}
\item Define initial cluster centroids
\item Partition data by assigning \emph{each sample} to the nearest centroid (2-norm)
\item \textbf{Recalculate} the centroids within each partition
\end{enumerate}
Repeat 2\&3 until clusters are invariant.
\begin{verbatim}
while centroid_displacement_from_previous_iteration() > eps:
    (sample.select_nearest_centroid() for sample in samples)
    recompute_centroids()
\end{verbatim}

\item Initialization
\label{sec-2-2-1-3-2}

Poor initialization may lead to improper clustering!

Different ways of doing it:
\begin{itemize}
\item Random selection of K centroids
\item Random partition of data
\item Select K points that are \emph{mutually far apart}
\item Domain-knowledge
\item Initialize using results from some other method
\item Average over many clustering runs
\end{itemize}


\item Cluster number
\label{sec-2-2-1-3-3}

We may have domain knowledge to inform a proper choice of K, but usually it can be determined from the data itself.

We \emph{can't} just pick the $K$ that minimizes $J$ since the ideal clustering is then to make \emph{each point its own cluster}.

A heuristic methods runs different K values and plots the corresponding J value. We can look at the "elbow" of the $K$ vs. $J$ plot, at which we don't get any additional benefit by adding more 

\item Advantages
\label{sec-2-2-1-3-4}

\begin{itemize}
\item Easy to implement
\item Often converges in very few operations
\item Can be applied on data with many features
\end{itemize}

\item Disadvantages
\label{sec-2-2-1-3-5}

\begin{itemize}
\item We must chose $K$, risking not having an optimal solution or being forced to perform multiple expensive runs.
\item Iterative algorithm returns \emph{local minima}.
\item By using the 2-norm, it assumes that all clusters are spherical and about the same size.
\item Very sensitive to outliers, as with any method that relies on means.
\end{itemize}

However, there are many variants that fix some of these issues.
\begin{itemize}
\item Running algorithm multiple times
\item Using medians instead of means
\item Require that the centroid \emph{must be a data point}
\begin{itemize}
\item Robust to outliers
\item Flexible -- can use any similarity measure
\item Computationally expensive to calculate the mean
\end{itemize}
\end{itemize}

\item Possible modifications
\label{sec-2-2-1-3-6}

Many possible modifications, like weighing the distance by some metric. For example, if we know a particular subset of a dataset is noisy, we can downplay its importance by weighing its distance to the centroid less than we weigh other subsets.
\end{itemize}
\end{itemize}



\subsubsection*{Dimensionality Reduction}
\label{sec-2-2-2}

For visualization and data intuition purposes it's necessary to wrangle higher dimensions into human-understandable terms. This is basically the science of making good projections onto lower-dimensional spaces. We \emph{always} lose information.


\begin{itemize}
\item Maximal Variance Projection (PCA)
\label{sec-2-2-2-1}

Usually we'll chose hyperplanes that \emph{retain the spread} of the data, i.e. maximizing variance. This is \textbf{Principal Component Analysis}.

If we have a 2D oval cluster, projecting onto the line that goes through the major axis will maximize the variance. This line is found through either the:
\begin{itemize}
\item Sample Covariance matrix: $X^TX=VD^2V^T$
\item Singular Value Decomposition: $X=UDV^T$
\end{itemize}
Where X: Data matrix, V: Eigenvalue matrix, D: Eigenvector matrix

That is, the eigenvector that corresponds to the \emph{largest eigenvalue} is this best-projection line. These eigenvalues are the variances!

Check out the paper "Genes mirror geography within Europe". Very high-dimensional data, when projected on 2D using PCA, \emph{approximates Europe's geography!}

\begin{itemize}
\item Steps:
\label{sec-2-2-2-1-1}

\begin{itemize}
\item Centering (always) -- Subtract the centroid from each data point to center it at the origin.
\item Normalizing (used for correlation PCA; not the same thing)
\item Whitening (never) --  Standarize units by dividing by standard deviations.
\end{itemize}


\item Differences with Linear Regression
\label{sec-2-2-2-1-2}

A linear regression minimizes the square of the \emph{vertical} distance between some sample and the regressor. However, the PCA fit minimizes the total \emph{euclidean} distance between data and fit.


\item Choosing Principal Components
\label{sec-2-2-2-1-3}

We'll usually choose 2 PCs to visualize data in paper media or presentations.

We can measure how much of the variance is lost. Recall that the eigenvalues capture the variance of each PC eigenvector! If we plot a PC vs. $\sigma^2$ histogram, we can look for steep drop-offs. If we don't find these, there's a lot of information we're losing by reducing dimensionality. In these cases, be careful of finding degenerate eigenvectors (equal eigenvalues) -- both of these must be present, as any linear combination of these is also valid

\begin{itemize}
\item Analytical Method
\label{sec-2-2-2-1-3-1}

Variance explained: $W_k = \frac{1}{N}\sum_{i=1}^{k} {d_i^2}$ (Average square error)


Gap Statistic: $\log W_k - \log W_k^*$


$W_k^*$ is the variation found from random data (same N) simulated inside some bounding region (bounding box, convex hull, etc). By plotting the gap statistic, we measure how much \emph{better} our fit works on the data than with random data. The maximum gives us the optimum k.


This is also used in k-means with the $J$ metric instead of $W$.
\end{itemize}


\item Dangers
\label{sec-2-2-2-1-4}

The first PC is not always the best projection! Imagine a data set with parallel oblique clusters -- the first PC is parallel to these clusters, as it captures the most spread. However, by projecting onto it, the clusters merge and we'll think our data is completely structureless!
\end{itemize}


\item Self Organizing Maps
\label{sec-2-2-2-2}

If we find a hypercurve that best fits the data, it's possible to describe all the data with a single parameter. However, finding an arbitrary curve is a rather difficult problem.

If we define an initial straight curve (first principal component, usually) defined by N points $m_k$, we iterate by making every sample point $x_i$ \emph{pull} the closest $m_k$ towards itself, proportional to how far apart they are. This is described by the relation $m_k \leftarrow m_k + \alpha\left(x_i - m_k\right)$. The fact that the pulling action is averaged over all the nearby data points is key for preserving the curve structure.

The parameter $\alpha$ is called the \emph{learning rate}, and is usually best set to a small value to approach the solution smoothly through lots of iterations rather than jerking back and forth and risk accuracy.


\item Multi-Dimensional Scaling (MDS)
\label{sec-2-2-2-3}

By defining distance metrics between variables (can be esoteric), we record these distances in a distance map which we can reconstruct using least-squares minimization. The idea of reconstructing maps from 'road distances', for example, is viable! The resulting map might be arbitrarily rotated, but it will capture geometrical relationships very well.


These are affine spaces in which we can't define an origin but we can define distances.


$\min_{x_1,...,x_N} \sum_{i<j} \left(||x_i-x_j||-\delta_{ij}\right)^2$


\item Independent Component Analysis (ICA)
\label{sec-2-2-2-4}
Best used when data is a linear superposition of signals, like with audio. Let's assume we have three microphones in a room and we want to isolate each independent sound source out of three: TV, Radio, Speech.

We have a data matrix with each variable (microphone) over time which we'll whiten/normalize. For this we compute all PCs, and scale along each of these by the according eigenvalue, after which direcions become uncorrelated but not independent.

From this, we want to the lowest entropy projection, or that which is non-Gaussian. This is, that it has \emph{structure} as measured by some entropy metric. Projecting on these will ensure that we maintain structure, rather than just variance.

An important limitation is that you can only find as many projections as you have 'measuring devices'.

This has applications in finding patterns in financial data, earth monitoring, etc.
\end{itemize}

\section*{Day 2}
\label{sec-3}
\subsection{Measuring algorithm performance}
\label{sec-3-1}
Loss functions quantify cost of errors. E.g: Mean Squared Error (MSE) = $\frac{1}{N}\sum\left(y_i-f(x_i)\right)^2$

Keep in mind that we don't want to minimize the error on the data we have (that leads to overfitting. Rather, we want to minimize the error on \emph{new data}, such that we build a model that \emph{generalizes}, rather than just fit the trends.

In the end, we want to minimize the \emph{expected loss} on future data.

Training error and Test error behave differently. As our model flexibility increases, the training error always decreases, but past a certain point the test error will actually begin to increase!

When the number of degrees of freedom approaches the size of the data we get into dangerous overfitting territory. Complex models like deep neural networks involve millions of parameters, though at the same time they require far larger datasets to train.
\subsubsection*{Cross Validation}
\label{sec-3-1-1}
Divide the data into three subsets:
\begin{itemize}
\item Training data: Subset used to learn the model
\item Validation data: Subset used to estimate error for tuning or model selection
\item Test data: Subset used to check model performance. This has not been previously been used on the model.
\end{itemize}
\begin{itemize}
\item Bias vs. Variance Tradeoff
\label{sec-3-1-1-1}
The expectation of the error for the given estimator (i.e., model) is given by

$\text{Variance(}f\hat\text{)} + \text{Bias(}f\hat\text{)}^2 + \text{Variance(}\varepsilon\text{)}$

The bias represents the ability of the model to represent the actual trend. For example, trying to linearly fit a nonlinear model is biased towards the linear fit wheras a slightly more complex model would fit the data better.

Variance, on the other hand, quantifies how much the estimation varies with different datasets.

As we increase the DOFs, bias is reduced, but at the same time variance is increased.

Ideally, low bias \emph{and} low variance imply a low test error.
\end{itemize}
\subsection{Classification}
\label{sec-3-2}
\subsubsection*{K-Nearest Neighbor classifier}
\label{sec-3-2-1}
Basic idea: classify observations based on nearby labels.

The predicted class for a sample X is the most common class among its K nearest neighbors from the training set.

The probability of belonging to some class is quite easily $\frac{\#\text{(class among neighbors)}}{K}$.

Decision boundaries are a useful way of visualizing the regions of classification as well as noting the complexity of some fit.

This is a \emph{non-parametric} model, as we're not fitting an equation with fixed parameters.

Useful for imputating data!
\begin{itemize}
\item Choosing K
\label{sec-3-2-1-1}
With small K, we overfit and the decision boundary is very rough and jittery. However, with a large K the data might not be abundant enough to capture \emph{any} trends.

Again, by minimizing the expected error we can choose the optimal K.
\item Advantages
\label{sec-3-2-1-2}
\begin{itemize}
\item Simple to implement
\item Few tuning parameters (K, distance metric)
\item Flexible, doesn't impose linear separability
\end{itemize}
\item Disadvantages
\label{sec-3-2-1-3}
\begin{itemize}
\item Computationally expensive
\item Sensitive to imbalanced datasets (larger classes smother the smaller ones)
\item Sensitive to irrelevant inputs
\end{itemize}
\end{itemize}
\subsubsection*{Regression}
\label{sec-3-2-2}
\begin{itemize}
\item Linear Regression
\label{sec-3-2-2-1}
It's a simple supervised learning method! Its advantage is \emph{highly interpretable} as the slope parameters easily quantify the impact of individual variables.
\begin{itemize}
\item Simple Linear Regression
\label{sec-3-2-2-1-1}
Parametric model given by $Y=\beta_0 + \beta_1 X + \varepsilon$.

We estimate \$\hat$\beta$$_{\text{0}}$ and \$\hat$\beta$$_{\text{1}}$ using training data to find a good fit, and for this we usually use the Mean Squared Error.

SLR in particular has a closed-form solution for the parameters that gives the best least-squares fit.
\item Multiple Linear Regression
\label{sec-3-2-2-1-2}
We use more than one predictor (X) variable on the form Y=$\beta$$_{\text{0}}$+$\beta$$_{\text{1}}$ X+$\beta$$_{\text{2}}$ X$_{\text{2}}$+\ldots{}

Using the magic of linear algebra, we may define X and $\beta$ vectors to set up the optimization problem \hat$\beta$ = $\arg$$\min$$_{\beta}$ ||Y - X$^{\text{T}}$$\beta$ ||$^{\text{2}}$. This also has a closed-form solution (found through the \emph{normal equations}) given by $\beta = \left(X^TX\right)^{-1}X^TY$

\item Variations on Linear Regression
\label{sec-3-2-2-1-3}
\begin{itemize}
\item Weighted Linear Regression
\label{sec-3-2-2-1-3-1}
Not all data is created equal -- if we know a particular subset of data is less reliable, i.e. more noisy, we reduce its impact on the model through weighting.

Using a diagonal matrix $W$ with elements \$w$_{\text{i}}$ = \frac{1}{\sigma_i^2} where each $\sigma$$_{\text{i}}$ may be esoteric or properly measured. With this, the problem becomes
\item Locally weighted linear regression
\label{sec-3-2-2-1-3-2}
We pressume an \emph{a priori} interest in some particular region of data. For some observation, we apply a gaussian weighing surrounding the point of interest to produce a sort of tangent line.

A new linear fit must be produced for \emph{every} desired observation, and in the end this produces a piece-wise linear approximation to the trend curve.
\end{itemize}
\end{itemize}
\item Logistic Regression
\label{sec-3-2-2-2}
\begin{itemize}
\item Qualitative inputs
\label{sec-3-2-2-2-1}
Class predictors like KNN are based on purely categorical data. However, it's possible to classify \emph{using regression} by regressing probabilities of belonging to different classes.

When input data is qualitative, (e.g. "female", "male", "blue eyes", "brown eyes") it's easy to use binary variables that quantify this. However, it's dangerous to apply a linear regression with categories assigned to different numbers (i.e. symptoms: headache:1, seizure:2, stroke:3) to any categories as this introduces unnatural orderings that bias the fit.

The best way of dealing with categorical data is using \emph{binary variables}. Either 1 or 0 whether the category is true or not.
\item Description
\label{sec-3-2-2-2-2}
$Y$ takes on two values: 0 or 1, and we estimate it with probabilities in the interval $[0-1]$. 

We use the sigmoid/logistic function $\sigma(z)=\frac{1}{1+e^{-z}}$ that lies in the desired interval. By making $z$ the linear regressor (Xs aren't forced to be binary), we can "squash" the data to perform categorical fitting.

By choosing the sub-space where $\mathbb{P}=p$ we can define the decision boundary for the classification, so instead of fitting a line, we're fitting a sort of \emph{step} function.
\item Advantages
\label{sec-3-2-2-2-3}
\begin{itemize}
\item Extension of Linear Regression
\item Interpretability -- log-odds are linear
\item No tuning of hyperparameters
\end{itemize}
\item Disadvantages
\label{sec-3-2-2-2-4}
\begin{itemize}
\item Can't model complex decision boundaries
\item May overfit in training data, though it can be mitigated with Regularization in the MLE method
\item Problem \textbf{must} be formulated as binary classification
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Cross-Validation and Regularization}
\label{sec-3-3}

\subsubsection*{Cross-Validation}
\label{sec-3-3-1}

\begin{itemize}
\item XKCD example with the jelly beans and medical research
\label{sec-3-3-1-1}
\href{https://xkcd.com/882/}{Statistical significance} has no meaning if we don't properly manage our p-values to account for the fact that we're bombarding the data with the same models. If we get results, it's probably because we arrived at the $p%$ of times that we get it from pure chance!
\item {\bfseries\sffamily TODO} Loss Functions
\label{sec-3-3-1-2}
In linear regression, why do we use the squared error instead of just the absolute error? Usually it's because it gives us a closed form solution.

\begin{itemize}
\item Squared Error
\label{sec-3-3-1-2-1}
$\sum_j\left(Y-f(X)\right)^2$

This is affected much more by outliers, as they make the square error far larger than the simple distance. This error function leads to the mean, which presents high sensitivity.
\item Absolute Error
\label{sec-3-3-1-2-2}
$\sum_j\left|Y-f(X)\right|$

Since there's no further penalization of far-away terms, this loss function is less sensitive to outliers. This gives rise to the median, a far more stable central measure.
\end{itemize}

\item {\bfseries\sffamily TODO} Classification Loss Functions
\label{sec-3-3-1-3}
Indicator error (nuumber of times "I screwed up", i.e. classifying spam(1) as not-spam(0)).


"Flip" what we're saying: If we get something wrong 80\% of the time, just flip the classification!

\begin{itemize}
\item {\bfseries\sffamily TODO} Misclassification
\label{sec-3-3-1-3-1}

Gini index of uniformity
\end{itemize}

\item {\bfseries\sffamily TODO} Supervised Learning Theory
\label{sec-3-3-1-4}

Given a loss function $L(Y, \tilde f(X))$, our goal is to find $\hat f = \argmin_{\tilde f} \mathbb{E}[L(Y, \tilde f(X))]$. That is, across the population $X$, find the model $\tilde f$ that minimizes the expected error.

\item K-fold Cross Validation
\label{sec-3-3-1-5}
We divide our data into different subsets, some of which are used for testing and some of which are used to \emph{validate} the model. We can reshuffle the dataset and do this multiple times to get an average.

Cross-Validation Error: (tests whether a \textbf{\emph{type}} of model is a good fit, rather than a particular fit). This cannot be used to replace the necessity for validation sets!

N-fold cross-validation is the best validation method -- it leaves one sample out and uses everybody else to train and later validate one the one sample. It performs this N times, meaning it's very expensive for more complex models.

Good compromises are 5-fold and 10-fold cross valdiations.

\emph{Types} of models can be understood as different parameters for a single model, e.g. 2 neighbors for KNN vs 3 neighbors, etc.

\item Bias-Variance tradeoff
\label{sec-3-3-1-6}
See \hyperref[sec-3-1-1-1]{Previous Heading}.

\item Learning Curves
\label{sec-3-3-1-7}
How do we answer the question: do we need a better model or do we need more data?

Compare the training error and Cross-validation error as the data size changes \emph{with the same model}. As we increase the data size, the same fit isn't as able to fit newer data so the \emph{training error increases} as the cross-validation error decreases and eventually bottoms out. These errors never actually cross, but when the curves flatten out it's time to get a better model.

In summary, big gap implies we need more data, small gaps imply it's time to improve the model.

\item Common problems!!
\label{sec-3-3-1-8}
The test sets must \emph{never be used until testing}. We can't use them to extract information that will affect how we develop the models.

Be careful with using too many different models on the same test set. Eventually, a model will \emph{by chance} fit the model well.

\item {\bfseries\sffamily TODO} Importance of the validation set
\label{sec-3-3-1-9}
Only, and only when we've exhausted the model space and found a model that might

\item Training data issues
\label{sec-3-3-1-10}

\begin{itemize}
\item Data is biased!
\label{sec-3-3-1-10-1}
As an example, historical text incorporates sexism (4 times more male references to female ones, as well as other biases in content) that gets incorporated into automated systems unnoticed.

\begin{quote}
Machine Learning is like money laundering for bias. It's a clean, mathematical apparatus that gives the status quo the aura of logical invenitability
--Maciej Ceglowski
\end{quote}
\end{itemize}
\end{itemize}

\subsubsection*{Regularization}
\label{sec-3-3-2}
"Too many cooks spoil the soup"

Many models for regression involve many variables or so-called "proprietary" variables -- functions of the other variables.

\begin{itemize}
\item Problems with models
\label{sec-3-3-2-1}
\begin{itemize}
\item More variables than samples
\label{sec-3-3-2-1-1}
Having too few samples for very high-dimensional models allow us to fit an infinitude of spaces. For example fitting two samples to a three dimensional model lets us fit infinitely many planes that pass through the line connecting the samples.
\item Multiple-Colinearity
\label{sec-3-3-2-1-2}
What can go wrong when we have a few variables that are highly correlated between each other but not with the variable we're trying to predict? When performing a fit, these variables won't affect the predictor and might happen to cancel out mutually -- because of this, their weights might become disproportionally large, making it seem like these variables are actually very important when in fact they're not!
\end{itemize}


\item Ridge Regression
\label{sec-3-3-2-2}
Incorporate a 'penalized error' $\lambda \left(\sum\beta_i^2\right)$. 

As $\lambda$ increases, the \emph{bias} increases as we end up simplifying the model ($\beta$$_{\text{i}}$ tend to zero), and the variance decreases as we always reach the same estimations.

\item Lasso
\label{sec-3-3-2-3}
How do we choose which variables can be tuned to \emph{exactly} zero?

"Least Absolute Shrinkage and Selection Operator"

Rather than squaring the betas, we use their absolute value. Thus, the penalty is $\lambda\left(\sum|\beta_i|\right)$

This becomes an L1 problem rather than an L2 problem used in Ridge Regression. It doesn't find smooth values very well but it approaches zero very reliably. Because of this, our data is able to become sparse much more reliably.
\end{itemize}

\section*{Day 3}
\label{sec-4}

\subsection{Back to Regularization}
\label{sec-4-1}
\subsubsection*{Ridge Regression vs. Lasso -- Sparsity}
\label{sec-4-1-1}
The general formulations for ridge regression and lasso problems are:
\begin{itemize}
\item RR: $\beta = \arg\min_{\beta}\sum\left(Y - \beta_0 - \sum\beta_i X_i\right)^2$
\item Lasso:
\end{itemize}

However, using Lagrange Multipliers, the problems may be formulated as:
\begin{itemize}
\item RR:
\item Lasso:
\end{itemize}

If we visualize these constraints, it's easy to see that a circle intersects with the error function at a point distinct from the axes -- the optimum is different than zero. On the other hand, the l1 constraint with its diamond shape contacts the error exactly \emph{at} the axis! The other axes (coefficients) thus end up as zero.

A $q$ value smaller than 1 would improve sparsity, but the problem is no longer convex and 'easy' to solve.

As we increase $\lambda$, coefficients might not decrease monotonically -- as we decrease responsibility in some coefficients, some coefficients are in turn assigned more responsibility.

\subsubsection*{Bayesian priors}
\label{sec-4-1-2}

\subsubsection*{{\bfseries\sffamily TODO} Elastic Net}
\label{sec-4-1-3}
When combining ridge regression and lasso we get the Elastic Net method:

\begin{itemize}
\item $\beta = \arg\min_{\beta}$
\end{itemize}

\subsubsection*{Principal Components Regression}
\label{sec-4-1-4}
Instead of using the original variables for the regression, we reduce the number of variables by projecting onto principal components and using the \emph{distances} between each sample and PC as the variables for regression.

\subsection{Support Vector Machines}
\label{sec-4-2}
Classification algorithm for \emph{binary} classification. It's a generalization of a \emph{maximal margin classifier}, l

\subsubsection*{Maximal Margin Classifier}
\label{sec-4-2-1}
Assumption that data is \emph{linearly separable}, i.e. there exists a linear decision boundary --a hyperplane-- that separates the two classes.

In many cases there are many possible candidate hyperplanes -- which one do we choose?
\begin{itemize}
\item Maximize the distance between class samples and the hyperplane! This way, when we perform a classification, there's less of a chance that the new observation will have belonged in the other class rather than the chosen one.
\end{itemize}

The "margin" is the smallest distance between \emph{any} training observation and the hyperplane, such that the "maximal margin" is the sort of "widest road" between the classes.

The \emph{support vectors} are the training samples that are equidistant from the hyperplane -- changing these will change the hyperplane, but changing anything else won't. They're labeled \emph{support} as the hyperplanes depends on them and them only.

Disadvantages:
\begin{itemize}
\item Sensitive to individual observations
\item May overfit the training data!
\end{itemize}

Now, what happens if there's no linear separability?

\subsubsection*{Support Vector Classifier}
\label{sec-4-2-2}
This model allows some samples to be on the 'wrong side' of the margin or hyperplane. 

We loosen the constraints to allow some samples to be in the middle of the 'road'. However, we still impose penalties using budgeted 'slack variables' -- these allow violations of the margin. 

A slack variable $\varepsilon$$_{\text{i}}$ is such that:
\begin{itemize}
\item $\varepsilon$$_{\text{i}}$ = 0 when datum is in correct side of the hyperplane
\item $\varepsilon$$_{\text{i}}$ > 0 when datum is on wrong side of the margin
\item $\varepsilon$$_{\text{i}}$ > 1 when datum is outright misclassified
\end{itemize}

Having fewer support vectors implies higher variance -- once again we must control the tradeoff!

However, when data is way non-linear this is still useless!

\subsubsection*{Support Vector Machines}
\label{sec-4-2-3}
The main trick is to increase the dimensionality of the data! Add "proprietary" variables such as polinomial combinations of variables. On these higher-dimensional spaces, the data can be linearly separable!

We could define these variables by hand, but they might not always be the best choice, and using exhaustive approaches can quickly make our new dataset hard to handle!

SVMs do this more or less automatically -- they use \emph{kernels} which implicitly map data into higher-dimensional space and then apply support vector classifiers.

\begin{itemize}
\item Kernels
\label{sec-4-2-3-1}
Kernels are generalizations of the inner product -- they give a measure of similarity between two points.

The main advantage is in being able to represent these as a \emph{feature map} which significantly improves the computability.

\begin{itemize}
\item Linear Kernel
\label{sec-4-2-3-1-1}
$K(X,X') = <X, X'>$
Equivalent to just using Support Vector classifiers!

\item Polynomial Kernel
\label{sec-4-2-3-1-2}
\$K(X, X') = (1 + <X, X'>) \$

\item Radial Basis Kernel
\label{sec-4-2-3-1-3}
\$K(X,X') = $\exp$ (- $\gamma$ ||X - X'||$^{\text{2}}$) \$
\end{itemize}

\item Advantages
\label{sec-4-2-3-2}
\begin{itemize}
\item Regularization parameter C to avoid overfitting
\item Kernels give flexibility in the shape of the decision boundary
\item Convex optimization problem -- unique solution
\end{itemize}
\item Disadvantages
\label{sec-4-2-3-3}
\begin{itemize}
\item Must tune hyperparameters -- poor performance if not well chosen
\item Hard to interpret
\item Problems \emph{must} be formulated as binary classification
\end{itemize}
\end{itemize}
\subsubsection*{SVMs for more classes?}
\label{sec-4-2-4}
Two main approaches
\begin{itemize}
\item One-versus-one
\label{sec-4-2-4-1}
Classifies every pair of classes
\item One-versus-all
\label{sec-4-2-4-2}
\end{itemize}

\subsection{Imputation on Missing Data}
\label{sec-4-3}

\subsubsection*{Imputation}
\label{sec-4-3-1}
If we threw away every survey that was missing some data, we'd end up throwing away most of our dataset! \emph{Imputation} is the process of filling in missing data.

\emph{Inferring computation}.

Oftentimes, imputation is part of the data wrangling preprocess, in the way of some further analysis, However, sometimes imputation is the whole point! \emph{Recommender} systems are purely imputation systems usually based in matrix completion.

\begin{itemize}
\item Missing Completely At Random (MCAR)
\label{sec-4-3-1-1}
Data might be missing \emph{because} it doesn't fit some pattern -- we must ensure that the data that's missing is missing purely randomly.

For example, voting turnout data might be linked to geographies -- If we know some geographical region has certain tendencies and yet it has low turnout in some election, it's not valid to assume that missing votes are MCAR.

If we're able to build a classifier that can predict whether data is missing, then the data is \textbf{not} missing at random!

\item Methods
\label{sec-4-3-1-2}
\begin{itemize}
\item Deleting incomplete observations
\label{sec-4-3-1-2-1}
Not good -- we end up losing most of our data!
\item Filling in with the mean or median of the data
\label{sec-4-3-1-2-2}

\item KNN imputation
\label{sec-4-3-1-2-3}
\texttt{impute} command in R

\begin{enumerate}
\item Fill-in missing values with mean or median for those variables
\item Compute the distance between the \emph{observation} missing a value and all others to find the k closest observations (using \emph{all} variables)
\item Make sure you ignore the variable that's missing the value when computing distances.
\item Use the k neighbors to compute a new mean or median
\end{enumerate}

The reason we first fill-in the missing values is such that we're able to compute distances between observations that may have values missing for variables \emph{other} than the one we're trying to fill in.
\item SVD imputation
\label{sec-4-3-1-2-4}
\begin{enumerate}
\item Initalize missing data however
\item Use a rank-k SVD to down-dimension and plug in the missing data with the projected value
\item Repeat until convergence
\end{enumerate}
\end{itemize}
\item Performance on highly sparse datasets
\label{sec-4-3-1-3}
How do we determine how well we do on the imputation?

Similarly with the gap statistic in K-means, we cross-validate with random data as we compare models.

With imputation, we need \emph{high bias}, as there's such a lack of data that high variance is bound to overfit.
\end{itemize}
\subsection{Decision Trees -- Classification and Regression Trees (CART)}
\label{sec-4-4}
\emph{Recursive binary} classifier. We continuously subdivide a region based on yes or no conditions, which results in \emph{highly interpretable} representations of the data.

A typical decision function is of the form $f(x) = \sum(c_m I(x\in R_m))$, where $I$ is the indicator function, equal to 1 if the condition is fulfilled and 0 if not. The different values of $c_m$ code the result for particular combinations of regions.

In typically decision trees, the branch length encodes the importance of a particular categorization.
\subsubsection*{Creation algorithms}
\label{sec-4-4-1}
\begin{itemize}
\item Greedy Algorithm
\label{sec-4-4-1-1}
Start with a single split and optimize the one variable and splitting point that minimizes the prediction error. Now consider each subtree as its own tree and repeat!

If our variable is categorical, we can \emph{aggregate} their underlying variables and order its values.
\end{itemize}

\subsubsection*{Depth of the tree?}
\label{sec-4-4-2}
Splitting exhaustively is a bias-variance issue -- if the last category is the training sample itself, the prediction is perfect and we have overfitting!

\begin{itemize}
\item Stopping Criteria
\label{sec-4-4-2-1}

\begin{itemize}
\item Minimum decrease in error -- stop on short branches
\label{sec-4-4-2-1-1}
Not good since an initial poor prediction can enable a much better prediction, but we wouldn't know since we stopped expanding.

\item Pruning
\label{sec-4-4-2-1-2}
We initially grow out the tree completely and later prune the insignificant splits until some desired complexity.
\end{itemize}
\end{itemize}

\subsubsection*{Advantages}
\label{sec-4-4-3}
\begin{enumerate}
\item Handles missing data through surrogate splits
\item Robust to non-informative data
\item Automatic variable selection -- no regularization needed!
\item Very, very interpretable
\item Captures high order interactions
\end{enumerate}

\subsubsection*{Disadvantages}
\label{sec-4-4-4}
\begin{enumerate}
\item Instability of trees
\begin{itemize}
\item Solution: Random Forests
\end{itemize}
\item Lack of smoothness!
\begin{itemize}
\item Solution: MARS
\end{itemize}
\item Hard to capture additivity
\begin{itemize}
\item Solution: MARS or MART
\end{itemize}
\end{enumerate}
\subsection{Ensemble Methods}
\label{sec-4-5}
Condorcet's Jury Theorem:
\begin{itemize}
\item If the probability of a single individual being 'correct' is greather than 0.5, the aggregation of many individuals makes the total jury decision approach probability 1.
\end{itemize}

If some machine learning model has a higher-than-random-chance capability of prediction, using \emph{many} of them will always improve our predictions!

Averaging reduces variance but it doesn't increase bias!

However, there's a danger in that correlated classifiers don't help as much -- if the NYT tells people to vote for someone, we no longer have N independent, informed voters; we now have just one.

\section*{Day 4}
\label{sec-5}
\subsection{Random Forest and other Ensemble methods}
\label{sec-5-1}
\subsubsection*{Monte Carlo methods}
\label{sec-5-1-1}
Many problems do not have closed form solutions, e.g. the mean of the sum of two Negative-Binomial distributed random variables.
\subsubsection*{Bootstrap}
\label{sec-5-1-2}
It's physically imposible for us to pull ourselves up from our bootstraps -- however, it's viable to use the same data to generate more data! Useful when we don't have enough data for Monte Carlo methods!

Sample from distributions generated from known data, i.e. sample from the \emph{histograms}! This isn't any more complicated than simply sampling from the dataset. However, we must take care to put our samples \emph{back} into the original set else we change the shape of the distribution. \uline{Sampling with replacement}.

It doesn't matter that we end up with duplicates in the sampled dataset.
\subsubsection*{Bagging}
\label{sec-5-1-3}
\emph{Bootstrap aggregation}

Consider CARTS -- a different subset of the data might result in a widely differing tree from that generated from another subset. They're \emph{unstable}, even though the predictions aren't that much different.

The concept is to generate multiple predictors from a lot of bootstrapped datasets and aggregate their predictions. The reason this works is that erroneous answers are typically \emph{equally erroneous}, but those answers that are slightly more correct are correct in consensus!

A downside of this method is that we lose interpretability. A single tree is easy to understand, but how a thousand trees affect the prediction is quite a bit harder.
\begin{itemize}
\item Out-of-bag cross validation!
\label{sec-5-1-3-1}
There are some samples that are not included in the bootstrapped dataset --typically 1/3-- which can be naturally used for cross-validation.
\end{itemize}
\subsubsection*{Boosting}
\label{sec-5-1-4}
Fitting additional predictors to residuals, i.e. the \emph{errors}, from initial predictions. We adjust predictions using a percentage (decided with a learning rate $\lambda$) of some known errors.

Parameters:
\begin{itemize}
\item Tree number
\item Tree depth
\item $\lambda$ value
\end{itemize}
\subsubsection*{Random Forests}
\label{sec-5-1-5}
Bagged trees only have variability in the way that data is selected for the training. Thus, they can still be correlated amongst themselves, limiting their usefulness.

\emph{Random forests} introduce aditional randomness by \textbf{enforcing} the use of different splitting variables by randomly disallowing some percentage (chosen using CV). For example, bagged trees might be constantly misled into choosing a variable like 'age' over some other one, not allowing the model to find some better variable. This is the same idea as the concept of influenced, non-independent voters.

Because of this, each \emph{individual} tree tends to be worse, but the \emph{aggregate} is far better.

We still get OOB Cross-Validation!
\begin{itemize}
\item Variable importance measurements
\label{sec-5-1-5-1}
Measure whether some variable is important or not.
\begin{itemize}
\item Metric 1
\label{sec-5-1-5-1-1}
Average the total error decrease after a split involving this variable over all trees.
\item Metric 2
\label{sec-5-1-5-1-2}
Shuffle \emph{only the studied variable} and check whether prediction error increases. If it does, it turns out the variable was important!
\end{itemize}
\item Advantages
\label{sec-5-1-5-2}
\begin{itemize}
\item Very robust under non-informative variables -- less variance
\item Less prone to overfitting
\item No need for pruning!
\item Built-in cross validation sets with OOB
\end{itemize}
\item DIsadvantages
\label{sec-5-1-5-3}
\begin{itemize}
\item Hard to capture additive effects
\item Not interpretable
\end{itemize}
\end{itemize}
\subsection{SVM extras}
\label{sec-5-2}
\subsubsection*{Imbalanced classes}
\label{sec-5-2-1}
One class occurs far more often than the other, e.g. in fraud detection, medical databses.

Problematic because algorithms work best on even databases, and there's typically poor performance on underrepresented the class.

How do we imporove performance?
\begin{itemize}
\item Get more data for underrepresented class!
\item Class weighting
\begin{itemize}
\item Penalize more harshly making an error on the smaller class
\end{itemize}
\item Special sampling methods
\begin{itemize}
\item Modify training observations to balance class sizes, tipically by undersampling (+) or oversampling (-). This, however, may throw away important training observations or end up severly overfitting the model.
\end{itemize}
\end{itemize}
\begin{itemize}
\item Classification performance
\label{sec-5-2-1-1}
\end{itemize}
\subsection{Machine Learning for Text Data}
\label{sec-5-3}
\subsubsection*{Non-negative matrix decomposition}
\label{sec-5-3-1}
Non-negative matrix factorization $X=WH,W>=0,H>=0$ found by solving an optimization problem. $W$ becomes our basis conversion matrix, i.e. our dictionary, and $H$ contains the coefficients that represent the data.

\emph{Metafeatures}!

\begin{itemize}
\item {\bfseries\sffamily TODO} Advantages:
\label{sec-5-3-1-1}
\begin{itemize}
\item Interpretability thanks to non-negativity
\end{itemize}
\end{itemize}

\subsubsection*{Bag-of-Words model}
\label{sec-5-3-2}
Only store word frequency information.

\subsubsection*{N-gram}
\label{sec-5-3-3}
Contiguous sequence of words/characters, frequency counted similar to B.o.W. model.

\subsubsection*{Term-Document Matrix}
\label{sec-5-3-4}
A term may be either a word or a gram, representing each row in the matrix, whereby every column stores the frequency count for each term in some document

Red fish blue fish example

\begin{itemize}
\item {\bfseries\sffamily TODO} Red fish blue fish example
\label{sec-5-3-4-1}
\end{itemize}
\subsection{Review Questions}
\label{sec-5-4}
\begin{itemize}
\item In highly variable data, it's best to use restrictive models to mitigate overfitting. Only use more flexibility when dealing with data naturally of that kind, i.e. non-linear data.
\end{itemize}
\subsection{Crash Course on Neural Networks and Deep Learning}
\label{sec-5-5}
\subsubsection*{Neural Networks}
\label{sec-5-5-1}
Neural networks are modeled after physical brain neurons, but only inspirationally. Modern methods have lost most physical analogy.

Outputs of nodes are weighted differently as they become the inputs for other nodes.

We use non-linear \emph{activation functions} like ReLU, sigmoids, or hyperbolic tangents, so as to make the behavior of the network more complicated than a simple linear combination of weights.

Gradient descent is used to train the weights into minimzing some loss function assosciated with training data and predictions made. By itself, gradient descent is prone to not reaching global minima, though this can be useful for avoiding overfitting.

The mathematical theory of neural networks is severly underdeveloped! We use them from a technical perspective without understanding why they perform particularly well on certain datasets.

It's a field of tricks!

Regularization via \emph{early stopping} -- walk towards the minimum but don't actually reach it -- or \emph{dropout}, which motivates individual nodes to better respond to structures by themselves, rather than relying on other nodes'  behavior.

\subsubsection*{Deep Learning}
\label{sec-5-5-2}
Many many layers, many many parameters.

Only usable on \emph{obscenely} large datasets.
% Emacs 25.1.1 (Org mode 8.2.10)
\end{document}
